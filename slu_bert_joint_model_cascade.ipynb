{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU Joint Model Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Adrian Sarno, Jennifer Arnold\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2020\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all the random seeds for reproducibility. Only the\n",
    "# system and torch seeds are relevant for this notebook.\n",
    "import utils\n",
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.level = logging.ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace import\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# local imports\n",
    "import atis\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA test\n",
    "import sys; print(sys.version)\n",
    "import torch; print(torch.__version__, torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hf_weights_name = 'bert-base-cased'\n",
    "# hf_weights_name = 'bert-base-uncased' - in this case the tokenizer does not split into subwords so often\n",
    "\n",
    "hf_tokenizer = BertTokenizer.from_pretrained(hf_weights_name)\n",
    "hf_model = BertModel.from_pretrained(hf_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATIS_HOME = os.path.join(\"data\", \"atis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching (normalizing sentence lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encoder_vectorizer(input_sentences, max_length=None):\n",
    "    \"\"\"\n",
    "    This function accomplishes two tasks:\n",
    "    1.  tokenization and sentence-length normalization\n",
    "    2.  featurization, it calls the bert model to convert tokens to embeddings \n",
    "    \"\"\"\n",
    "    \n",
    "    # tokenization, encoding and sentence-length normalization\n",
    "    tokenizer_output = hf_tokenizer.batch_encode_plus(\n",
    "        input_sentences, \n",
    "        max_length=max_length,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True)\n",
    "    \n",
    "    input_token_ids = torch.tensor(tokenizer_output['input_ids'])\n",
    "    input_mask = torch.tensor(tokenizer_output['attention_mask'])\n",
    "\n",
    "    # featurization\n",
    "    with torch.no_grad():\n",
    "        final_hidden_states, cls_output = \\\n",
    "        hf_model(input_token_ids, attention_mask=input_mask)\n",
    "    \n",
    "    # cls_output not used\n",
    "    # convert to numpy to match the type of all other results (all numpy)\n",
    "    final_hidden_states = final_hidden_states.detach().cpu().numpy()\n",
    "    \n",
    "    return final_hidden_states, np.array(tokenizer_output['attention_mask']), np.array(tokenizer_output['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchJointSluModel(nn.Module):\n",
    "    def __init__(self,\n",
    "            embed_dim,\n",
    "            output_iob_dim,\n",
    "            output_intent_dim,\n",
    "            dropout_prob):\n",
    "        super(TorchJointSluModel, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.output_iob_dim = output_iob_dim\n",
    "        self.output_intent_dim = output_intent_dim\n",
    "        \n",
    "        # Graph\n",
    "        self.dropout = nn.Dropout(dropout_prob) \n",
    "        \n",
    "        self.iob_classifier_layer = nn.Linear(embed_dim, output_iob_dim)\n",
    "        self.init_weights(self.iob_classifier_layer) \n",
    "\n",
    "        self.intent_classifier_layer = nn.Linear(embed_dim, output_intent_dim)\n",
    "        self.init_weights(self.intent_classifier_layer) \n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        torch.nn.init.xavier_uniform(layer.weight) \n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.zero_()        \n",
    "            \n",
    "    def forward(self, X):\n",
    "    \n",
    "        # separate the feature vectors (embeddings) from the attention_mask\n",
    "        X, attention_mask = X\n",
    "\n",
    "        X = self.dropout(X)\n",
    "        \n",
    "        logits_iob = self.iob_classifier_layer(X)\n",
    "        \n",
    "        # create single embedding per sentence\n",
    "        sentence_X = X.mean(axis=1)  # for better performance\n",
    "        \n",
    "        logits_intent = self.intent_classifier_layer(sentence_X)\n",
    "\n",
    "        return logits_iob, logits_intent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_model_base import TorchModelBase\n",
    "from utils import progress_bar\n",
    "\n",
    "class TorchJointSlu(TorchModelBase):\n",
    "    \"\"\"\n",
    "     Featurization:\n",
    "        Takes the embeddings already pre-computed.\n",
    "    \n",
    "    Classification:\n",
    "        The simplest token classifier uses just a linear layer.\n",
    "        The Pytorch linear layer can take as input a tensor of any number of dimensions\n",
    "        and only the last dimension needs to be specified as input dimension.\n",
    "   \n",
    "        https://pytorch.org/docs/master/nn.html#linear   \n",
    "    \"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super(TorchJointSlu, self).__init__(**kwargs)\n",
    "        \n",
    "        self.config = config\n",
    "        self.input_embedding_size = config[\"input_embedding_size\"]\n",
    "        self.hidden_dropout_prob = config[\"hidden_dropout_prob\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.l2_strength = config[\"l2_strength\"]\n",
    "        self.max_iter = config[\"max_iter\"]\n",
    "        self.device = config[\"device\"]\n",
    "        self.class_weights = config.get(\"class_weights\", None)\n",
    "        if self.class_weights is not None:\n",
    "            class_weights = torch.FloatTensor(self.class_weights)\n",
    "        \n",
    "    def define_graph(self):\n",
    "        \"\"\"\n",
    "        This is a shallow model. so it does not really define a graph here \n",
    "        but it instantiates a model class with the classfier top.\n",
    "        \"\"\"     \n",
    "        self.num_iob_classes = len(self.iob_class2index)   # class2index is set in fit()\n",
    "        self.num_intent_classes = len(self.intent_class2index)   # class2index is set in fit()\n",
    "        print(f\"define_graph: num_iob_classes: {self.num_iob_classes}\")\n",
    "        print(f\"define_graph: num_intent_classes: {self.num_intent_classes}\")\n",
    "        return TorchJointSluModel(\n",
    "            self.input_embedding_size, \n",
    "            self.num_iob_classes, \n",
    "            self.num_intent_classes, \n",
    "            self.hidden_dropout_prob)\n",
    "\n",
    "    \n",
    "    def compute_loss(self, logits, attention_mask, labels):\n",
    "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            active_logits = logits.view(-1, self.num_iob_classes)\n",
    "            active_loss_mask = attention_mask.view(-1) == 1\n",
    "            active_labels = torch.where(\n",
    "                active_loss_mask, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "            )\n",
    "        else:\n",
    "            num_classes = logits.shape[-1]\n",
    "            active_logits = logits.view(-1, num_classes)\n",
    "            active_labels = labels.view(-1)\n",
    "\n",
    "        loss = loss_fct(active_logits, active_labels)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "            \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\"Standard `fit` method.\n",
    "        \n",
    "        fit() expects embeddings in X and strings in y.\n",
    "        The class itself is in charge of encoding the labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : [embeddings, attention_mask]\n",
    "        y : array-like, a list of lists of string [['O', 'B-fromcity']]\n",
    "        kwargs : dict\n",
    "            For passing other parameters. If 'X_dev' is included,\n",
    "            then performance is monitored every 10 epochs; use\n",
    "            `dev_iter` to control this number.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "\n",
    "        \"\"\"               \n",
    "        \n",
    "        ################################################################\n",
    "        # Model definition\n",
    "        ################################################################\n",
    "        \n",
    "        # Graph:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            self.compute_class2index(y)  # expects strings, must run before tensorizing y\n",
    "            self.model = self.define_graph()\n",
    "        \n",
    "        # Prime the model for training\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        \n",
    "        # Default is torch.optim.Adam\n",
    "        optimizer = self.optimizer(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.l2_strength)    \n",
    "          \n",
    "        \n",
    "        ################################################################\n",
    "        # Data \n",
    "        ################################################################\n",
    "        # separate the feature vectors (embeddings) from the attention_mask\n",
    "        X, attention_mask = X\n",
    "         \n",
    "        # Compute Incremental performance:\n",
    "        X_dev = kwargs.get('X_dev')\n",
    "        if X_dev is not None:\n",
    "            # X_dev contains 2 parameters, X_dev, attention_mask_dev\n",
    "            # the dev mask stays as numpy as is not used for anything\n",
    "            # because the class does not compute dev loss, it just stores \n",
    "            # the dev predictions. This mask is required by predict but\n",
    "            # it is just returned in the predict results (never used)            \n",
    "            dev_iter = kwargs.get('dev_iter', 10)\n",
    "            \n",
    "            \n",
    "        # separate the token labels from the sentence label\n",
    "        y_iob, y_intent = y\n",
    "\n",
    "        # encode labels (label vectorization). must run before tensorizing y\n",
    "        y_iob = self.encode_iob_labels(y_iob)\n",
    "        y_iob = self.pad_to_max_length(y_iob, X.shape[:2]) # y must have the shape of first 2 dims of X\n",
    "        y_intent = self.encode_intent_labels(y_intent)\n",
    "            \n",
    "        # cast data into PyTorch tensors\n",
    "        X = torch.tensor(X)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.bool)\n",
    "        y_iob = torch.tensor(y_iob, dtype=torch.long)\n",
    "        y_intent = torch.tensor(y_intent, dtype=torch.long)\n",
    "\n",
    "        # Wrap data into a dataset and use a Dataloader for batching\n",
    "        dataset = torch.utils.data.TensorDataset(X, attention_mask, y_iob, y_intent)\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, \n",
    "            shuffle=True, pin_memory=True) \n",
    "        \n",
    "\n",
    "        ################################################################\n",
    "        # Training process (Gradient Descent)\n",
    "        ################################################################\n",
    "        for iteration in range(1, self.config[\"max_iter\"]+1):\n",
    "            epoch_error = 0.0\n",
    "            for i, (X_batch, m_batch, y_iob_batch, y_intent_batch) in enumerate(dataloader):\n",
    "\n",
    "                X_batch = X_batch.to(self.device, non_blocking=True)\n",
    "                m_batch = m_batch.to(self.device, non_blocking=True)\n",
    "\n",
    "                logits_iob, logits_intent = self.model.forward(X=[X_batch, m_batch])\n",
    "        \n",
    "                # load the batch label tensors into GPU memory\n",
    "                y_iob_batch = y_iob_batch.to(self.device, non_blocking=True)\n",
    "                y_intent_batch = y_intent_batch.to(self.device, non_blocking=True)\n",
    "                \n",
    "                # compute the loss, the gradients and update the weights\n",
    "                err_iob = self.compute_loss(logits_iob, m_batch, y_iob_batch)\n",
    "                err_intent = self.compute_loss(logits_intent, None, y_intent_batch)\n",
    "                err = 0.5 * err_iob + 10 * err_intent\n",
    "                epoch_error += err.item()\n",
    "                # backprop pass\n",
    "                optimizer.zero_grad()\n",
    "                err.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Incremental predictions where possible:\n",
    "            if X_dev is not None and iteration > 0 and iteration % dev_iter == 0:\n",
    "                self.dev_predictions[iteration] = self.predict(X_dev)\n",
    "                self.model.train()\n",
    "            self.errors.append(epoch_error)\n",
    "            progress_bar(\n",
    "                \"Finished epoch {} of {}; error is {}\".format(\n",
    "                    iteration, self.config[\"max_iter\"], epoch_error))\n",
    "        return self\n",
    "\n",
    "\n",
    "    \n",
    "    def predict_flat(self, X):\n",
    "        \"\"\"Predicted classes for the examples in `X`. In flat format for metric functions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "        attention_mask: input mask\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probs: torch.tensor(batch_size * max_sequence_length, num_classes)\n",
    "        pred_class: numpy.array(batch_size * max_sequence_length)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        predict_output = self.predict(X)\n",
    "        \n",
    "        probs_iob, preds_iob = predict_output[\"probs_iob\"], predict_output[\"preds_iob\"]\n",
    "        \n",
    "        # Flatten and apply mask        \n",
    "        probs_iob_flat = probs_iob.flatten()\n",
    "        probs_iob_flat = probs_iob_flat[attention_mask.flatten() == 1]\n",
    "        \n",
    "        preds_iob_flat = preds_iob.flatten()\n",
    "        preds_iob_flat = preds_iob_flat[attention_mask.flatten() == 1]        \n",
    "        \n",
    "        return {\"probs_iob_flat\": probs_iob_flat, \n",
    "                \"preds_iob_flat\": preds_iob_flat, \n",
    "                \"probs_intent\": probs_intent, \n",
    "                \"preds_intent\": preds_intent, \n",
    "                \"attention_mask\": attention_mask}\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicted classes for the examples in `X`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "        attention_mask: np.array \n",
    "            input mask\n",
    "            # the dev mask stays as numpy as is not used for anything\n",
    "            # because the class does not compute dev loss, it just stores \n",
    "            # the dev predictions. This mask is required by predict but\n",
    "            # it is just returned in the predict results (never used)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probs: torch.tensor(batch_size, max_sequence_length, num_classes)\n",
    "        pred_class: numpy.array(batch_size, max_sequence_length)\n",
    "        attention_mask\n",
    "\n",
    "        \"\"\"\n",
    "        # get the attention_mask to return it\n",
    "        _, attention_mask = X\n",
    "\n",
    "        # compute probabilities and predicted class\n",
    "        probs_iob, probs_intent = self.predict_proba(X)\n",
    "\n",
    "        # compute predicted class, maximizing across the last dimension (classes)\n",
    "        _, pred_iob_class_idx = torch.max(probs_iob, dim=-1)\n",
    "        pred_iob_class_idx = pred_iob_class_idx.detach().cpu().numpy()\n",
    "        \n",
    "        _, pred_intent_class_idx = torch.max(probs_intent, dim=-1)\n",
    "        pred_intent_class_idx = pred_intent_class_idx.detach().cpu().numpy()\n",
    "        \n",
    "        # decode the class indices to class names (IOB_tag)\n",
    "        preds_iob = [[self.iob_index2class[i] for i in row_class_idx] \n",
    "                              for row_class_idx in pred_iob_class_idx]\n",
    "        preds_iob = np.array(preds_iob)\n",
    "\n",
    "        preds_intent = [self.intent_index2class[i] for i in pred_intent_class_idx]\n",
    "        preds_iob = np.array(preds_iob)\n",
    "        \n",
    "        # detach prob tensors\n",
    "        probs_iob = probs_iob.detach().cpu().numpy()\n",
    "        probs_intent = probs_intent.detach().cpu().numpy()\n",
    "\n",
    "        return {\"probs_iob\": probs_iob, \n",
    "                \"preds_iob\": preds_iob, \n",
    "                \"probs_intent\": probs_intent, \n",
    "                \"preds_intent\": preds_intent, \n",
    "                \"attention_mask\": attention_mask}\n",
    "        \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predicted probabilities for the examples in `X`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor(batch_size, max_sequence_length, num_classes)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Graph:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            # self.class2index must be defined in this case\n",
    "            self.model = self.define_graph()\n",
    "        \n",
    "        # prime the model for prediction-only mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            self.model.to(self.device)\n",
    "       \n",
    "            # cast input data into PyTorch tensors\n",
    "            x, attention_mask = X\n",
    "            x, attention_mask = torch.tensor(x), torch.tensor(attention_mask, dtype=torch.bool)\n",
    "\n",
    "            # load the input tensors into GPU memory\n",
    "            x = x.to(self.device)\n",
    "            attention_mask = attention_mask.to(self.device)\n",
    "            \n",
    "            # call forward \n",
    "            # (mask is not used unless you want to compute the training loss)\n",
    "            logits_iob, logits_intent = self.model.forward(X=[x, attention_mask])\n",
    " \n",
    "            # compute probabilities and predicted class\n",
    "            probs_iob = nn.Softmax(dim=-1)(logits_iob) # normalize scores along the latest dimension\n",
    "            probs_intent = nn.Softmax(dim=-1)(logits_intent) # normalize scores along the latest dimension\n",
    "            \n",
    "            return probs_iob, probs_intent  # tensor (no_grad)\n",
    "        \n",
    "    def compute_class2index(self, y):\n",
    "        \"\"\"\n",
    "        y: 2-D list of (lists of) strings (iob_tags, not indices)\n",
    "        \n",
    "        expects strings, must run before tensorizing y,\n",
    "        must run before defining the graph because it computes\n",
    "        the output network output (num_classes)\n",
    "        \n",
    "        Note:\n",
    "        if the input type is incorrect and it results in a number of classes is incorrect (very high) \n",
    "        it will likely cause a CUDA-OUT-OF-MEMORY error\n",
    "        \"\"\"\n",
    "        # separate the token labels from the sentence label\n",
    "        y_iob, y_intent = y\n",
    "        \n",
    "        # flat list of iob labels\n",
    "        iob_labels = []\n",
    "        for y_row in y_iob:\n",
    "            iob_labels.extend(y_row)\n",
    "            \n",
    "        # create iob mapping\n",
    "        iob_classes = sorted(set(iob_labels))\n",
    "        self.iob_class2index = dict(zip(iob_classes, range(len(iob_classes))))\n",
    "        self.iob_index2class = {i:c for c, i in self.iob_class2index.items()}        \n",
    "    \n",
    "        # create intent mapping\n",
    "        intent_classes = sorted(set(y_intent))\n",
    "        self.intent_class2index = dict(zip(intent_classes, range(len(intent_classes))))\n",
    "        self.intent_index2class = {i:c for c, i in self.intent_class2index.items()}        \n",
    "\n",
    "        \n",
    "    def encode_intent_labels(self, y_intent):\n",
    "        \"\"\"\n",
    "        y: 1-D list of (lists of) strings (intent)\n",
    "        \n",
    "        expects strings, must run before tensorizing y,\n",
    "        \"\"\"\n",
    "\n",
    "        return  [self.intent_class2index[label] for label in y_intent]\n",
    "        \n",
    "        \n",
    "    def encode_iob_labels(self, y):\n",
    "        \"\"\"\n",
    "        y: 2-D list of (lists of) strings (iob_tags, not indices)\n",
    "        \n",
    "        expects strings, must run before tensorizing y,\n",
    "        \"\"\"\n",
    "        tag_id_matrix = []\n",
    "        for iob_tags in y:\n",
    "            tag_ids = [self.iob_class2index[iob_tag] for iob_tag in iob_tags]\n",
    "            tag_id_matrix.append(tag_ids)  \n",
    "\n",
    "        return  tag_id_matrix\n",
    "    \n",
    "    def pad_to_max_length(self, jagged_matrix, output_shape):\n",
    "        padded_matrix = np.zeros(shape=output_shape)\n",
    "        for i, row in enumerate(jagged_matrix):\n",
    "            padded_matrix[i, :len(row)] = row \n",
    "        return padded_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Training and Prediction of the Shallow Slot Filling model (the Shallow model uses Bert embeddings without fine-tunning)\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling: Alignment, Encoding, Normalize the length of the sequences, Class Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(class_ids):\n",
    "    \"\"\"\n",
    "        class_ids: 1D tensor, contains one class_id for each example\n",
    "    \"\"\"\n",
    "    # encode the class_ids as onehot\n",
    "    class_matrix = np.zeros(shape=(len(class_ids), max(class_ids)))\n",
    "    class_matrix[class_ids] = 1\n",
    "    \n",
    "    # set the positive weights as the fraction of negative labels (0) for each class (each column)\n",
    "    w_p = np.sum(class_matrix == 0, axis=0) / class_matrix.shape[0]\n",
    "\n",
    "    # set the negative weights as the fraction of positive labels (1) for each class (each column)\n",
    "    w_n = np.sum(class_matrix == 1, axis=0) / class_matrix.shape[0]\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label and Sub-token Alignment (call WordPiece for each token, output word_to_tok_map and aligned_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_start_label_aligner(sentence, word_labels=None):\n",
    "    \"\"\"\n",
    "    Aligns the IOB labels to the word-starting tokens in the list of\n",
    "    sub-word tokens returned by the WordPiece tokenizer.\n",
    "    Returns:\n",
    "    - an array of indices, each pointing to the first sub-token of every word\n",
    "    - a padded list of labels, which has one element for each sub-token (the first\n",
    "      sub-token of every word gets the label, the rest get the padding label 'X')\n",
    "    \"\"\"\n",
    "    # Token map will be an int -> int mapping between the `word` index in the sentence and\n",
    "    # the WordPiece `tokens` index.\n",
    "    word_start_indices = []\n",
    "    tokens = [\"[CLS]\"]\n",
    "    if word_labels is not None:\n",
    "        token_labels = [\"O\"]\n",
    "    else:\n",
    "        token_labels = None\n",
    "    if len(sentence.split()) != len(word_labels):\n",
    "        print(f\"sentence: {len(sentence.split())}, word_labels: {len(word_labels)}\")\n",
    "        print(f\"sentence: {sentence.split()}, word_labels: {word_labels}\")\n",
    "    for word_idx, word in enumerate(sentence.split()):\n",
    "#     for word_idx, word in enumerate(WordPunctTokenizer().tokenize(sentence)):\n",
    "        word_start_indices.append(len(tokens))\n",
    "        word_tokens = hf_tokenizer.tokenize(word)  # tokenize ONE word \n",
    "        tokens.extend(word_tokens)\n",
    "        if word_labels is not None:\n",
    "            token_labels.append(word_labels[word_idx])\n",
    "            if len(word_tokens) > 1:\n",
    "                token_labels.extend([\"X\"]*(len(word_tokens)-1))\n",
    "\n",
    "    tokens.append(\"[SEP]\")\n",
    "    token_labels.append( \"O\")\n",
    "    return token_labels, word_start_indices, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_label_aligner(sentences, labels):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    label_matrix, word_start_matrix, token_matrix = [], [], []\n",
    "    \n",
    "    for sentence, word_labels in zip(sentences, labels):\n",
    "        token_labels, word_start_indices, tokens =\\\n",
    "            word_start_label_aligner(sentence, word_labels)\n",
    "        \n",
    "        label_matrix.append(token_labels)\n",
    "        word_start_matrix.append(word_start_indices)\n",
    "        token_matrix.append(tokens)\n",
    "\n",
    "    return label_matrix, word_start_matrix, token_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_predict_output(y_true, preds, attention_mask):\n",
    "    \"\"\"\n",
    "    y_true : list of list of strings (IOB_tags)\n",
    "        y_true is a list of  variable-length lists of strings, \n",
    "        is token-alignmed but not lenght-padded\n",
    "        \n",
    "    preds : np.array(batch_size, max_sentence_length)\n",
    "        2-D array with the class predicted by the model,\n",
    "        for each token of each sentence.\n",
    "        The attention mask must be applied to exclude the padding tokens.\n",
    "        \n",
    "    attention_mask: np.array(batch_size, max_sentence_length)\n",
    "        boolean tensor to filter the padding tokens\n",
    "    \n",
    "    In order to produce a classification report for sequence tagging, \n",
    "    first al the arrays need to be flattened.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"len(y_true): {len(y_true)}\")\n",
    "    print(f\"preds.shape: {preds.shape}\")\n",
    "    print(f\"attention_mask.shape: {attention_mask.shape}\")\n",
    "\n",
    "    # flatten the sequence labels\n",
    "    y_flat = []\n",
    "    for iob_tags in y_true:\n",
    "        y_flat.extend(iob_tags)\n",
    "    print(f\"y_flat (flattened): {len(y_flat)}\")\n",
    "\n",
    "    # apply mask to remove padding token positions and flatten the matrix\n",
    "    preds_flat = preds.flatten()\n",
    "    print(f\"preds_flat.shape (flattened): {preds_flat.shape}\")\n",
    "    preds_flat = preds_flat[attention_mask.flatten() == 1]\n",
    "    print(f\"preds_flat.shape (masked): {preds_flat.shape}\")\n",
    "    \n",
    "    return y_flat, preds_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_classification_report(y, predict_output, digits=3):\n",
    "    \"\"\"\n",
    "    Adapts the interface between the experiment and the sequence-tagging report function\n",
    "    y : non-padded token_label_matrix\n",
    "        list of list of strings (IOB_tags)\n",
    "        y is a list of  variable-length lists of strings, \n",
    "        is token-alignmed but not lenght-padded\n",
    "        \n",
    "    preds : np.array(batch_size, max_sentence_length)\n",
    "        2-D array with the class predicted by the model,\n",
    "        for each token of each sentence.\n",
    "        The attention mask must be applied to exclude the padding tokens.\n",
    "        \n",
    "    attention_mask:\n",
    "        boolean tensor to filter the padding tokens\n",
    "    \"\"\"\n",
    "    probs, preds, attention_mask = predict_output[:3]\n",
    "    print(f\"cr-probs: {probs.shape}\")\n",
    "    print(f\"cr-preds: {preds.shape}\")\n",
    "    print(f\"cr-attention_mask: {attention_mask.shape}\")\n",
    "        \n",
    "    y_flat, preds_flat =\\\n",
    "        flatten_predict_output(y, preds, attention_mask)\n",
    "\n",
    "    print(classification_report(y_flat, preds_flat, digits=digits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_macro_f1(y, predict_output, digits=3):\n",
    "    \"\"\"\n",
    "    Adapts the interface between the experiment and the sequence-tagging scoring function\n",
    "    y : non-padded token_label_matrix\n",
    "        list of list of strings (IOB_tags)\n",
    "        y is a list of  variable-length lists of strings, \n",
    "        is token-alignmed but not lenght-padded\n",
    "        \n",
    "        \n",
    "    preds : np.array(batch_size, max_sentence_length)\n",
    "        2-D array with the class predicted by the model,\n",
    "        for each token of each sentence.\n",
    "        The attention mask must be applied to exclude the padding tokens.\n",
    "        \n",
    "    attention_mask:\n",
    "        boolean tensor to filter the padding tokens\n",
    "    \"\"\"\n",
    "    probs, preds, attention_mask = predict_output[:3]\n",
    "    \n",
    "    y_flat, pred_flat =\\\n",
    "        flatten_predict_output(y, preds, attention_mask)\n",
    "    \n",
    "    return utils.safe_macro_f1(y_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Shallow SLU Joint Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_shallow_slu_joint_model(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    X_dev, \n",
    "    y_dev):\n",
    "    \"\"\"\n",
    "    fit() expects in X and strings in y.\n",
    "    The class itself is in charge of encoding (a.k.a. 'vectorizing') the labels.\n",
    "    \n",
    "    unit test the TorchShallowSequenceTagger.fit() method \n",
    "    This test calls the fit method on an untrained model \n",
    "    and later calls the metrics report on the dev results of the trained model\n",
    "    \"\"\"\n",
    "\n",
    "    # configure the sequence tagging layer\n",
    "    joint_config = {\n",
    "        \"input_embedding_size\": 768,\n",
    "        \"hidden_dropout_prob\": 0.4,\n",
    "        \"objective_weights\": None,  # loss penalizes more SF or ID\n",
    "        \"class_weights\": None,\n",
    "        \"batch_size\": 32,\n",
    "        \"lr\": 1e-3,\n",
    "        \"l2_strength\": 0,\n",
    "        \"max_iter\": 10,\n",
    "        \"device\": \"cuda\"\n",
    "    }\n",
    "    \n",
    "    # Shallow network\n",
    "    joint_model = TorchJointSlu(joint_config)   \n",
    "\n",
    "    # unpack train data\n",
    "    X_train, m_train = X_train\n",
    "    y_iob_tags_matrix_train, y_intent_train = y_train\n",
    "\n",
    "    # unpack eval data\n",
    "    X_dev, m_dev = X_dev\n",
    "    y_iob_tags_matrix_dev, y_intent_dev = y_dev\n",
    "    \n",
    "    # Fit\n",
    "    # we need to pass embeddings in X and strings in y\n",
    "    joint_model.fit(\n",
    "        X=[X_train, m_train],\n",
    "        y=[y_iob_tags_matrix_train, y_intent_train],\n",
    "        X_dev=[X_dev, m_dev],\n",
    "        y_dev=[y_iob_tags_matrix_dev, y_intent_dev]\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    predict_output = joint_model.predict(\n",
    "                        X=[X_dev, m_dev])\n",
    "    \n",
    "    probs_iob = predict_output[\"probs_iob\"]\n",
    "    preds_iob = predict_output[\"preds_iob\"]\n",
    "    attention_mask = predict_output[\"attention_mask\"]\n",
    "    \n",
    "    sequence_tagging_classification_report(y_iob_tags_matrix_dev, (probs_iob, preds_iob, attention_mask))\n",
    "    print(\"SF NON-VERBOSE MACRO-F1:\", sequence_tagging_macro_f1(y_iob_tags_matrix_dev, \n",
    "                                                             (probs_iob, preds_iob, attention_mask)))\n",
    "    \n",
    "     \n",
    "    print(\"\\n\\n\\nINTENT:\")\n",
    "    print(classification_report(y_intent_dev, predict_output[\"preds_intent\"]))\n",
    "    print(\"INTENT NON-VERBOSE MACRO-F1:\", utils.safe_macro_f1(y_intent_dev,  predict_output[\"preds_intent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    # read ATIS\n",
    "    atis_train = list(atis.train_reader(ATIS_HOME, class_func=atis.slot_filling_and_intent_func))\n",
    "    atis_dev = list(atis.dev_reader(ATIS_HOME, class_func=atis.slot_filling_and_intent_func))\n",
    "\n",
    "    # Split sentence and label\n",
    "    X_atis_sentences_train, y_atis_train = zip(*atis_train)\n",
    "    X_atis_sentences_dev, y_atis_dev = zip(*atis_dev)\n",
    "\n",
    "    # Split label iob and intent\n",
    "    y_iob_tags_train, y_intent_train = zip(*y_atis_train)\n",
    "    y_iob_tags_dev, y_intent_dev = zip(*y_atis_dev)\n",
    "\n",
    "    # Get token-level representations for all the input rows in the dataset\n",
    "    final_hidden_states_train, input_mask_train, input_token_ids_train =\\\n",
    "    batch_encoder_vectorizer(X_atis_sentences_train)\n",
    "\n",
    "    final_hidden_states_dev, input_mask_dev, input_token_ids_dev =\\\n",
    "    batch_encoder_vectorizer(X_atis_sentences_dev)\n",
    "\n",
    "    # align the labels to the sub-word tokens\n",
    "    y_iob_tags_matrix_train, _, _ =  sequence_tagging_label_aligner(X_atis_sentences_train, y_iob_tags_train)\n",
    "    y_iob_tags_matrix_dev, _, _ =  sequence_tagging_label_aligner(X_atis_sentences_dev, y_iob_tags_dev)\n",
    "    \n",
    "    # package data\n",
    "    X_train = (final_hidden_states_train, input_mask_train)\n",
    "    X_dev = (final_hidden_states_dev, input_mask_dev)\n",
    "    \n",
    "    y_train = (y_iob_tags_matrix_train, y_intent_train)\n",
    "    y_dev = (y_iob_tags_matrix_dev, y_intent_dev)\n",
    "    \n",
    "    return X_train, X_dev, y_train, y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_shallow_slu_joint_model():\n",
    "    X_train, X_dev, y_train, y_dev = read_dataset()\n",
    "\n",
    "    evaluate_shallow_slu_joint_model( X_train, y_train,\n",
    "                                      X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_shallow_slu_joint_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertJointSluModel(nn.Module):\n",
    "    def __init__(self,\n",
    "            output_iob_dim,\n",
    "            output_intent_dim,\n",
    "            dropout_prob,\n",
    "            weights_name='bert-base-cased'):\n",
    "        super(BertJointSluModel, self).__init__()\n",
    "        \n",
    "        self.weights_name = weights_name\n",
    "        self.output_iob_dim = output_iob_dim\n",
    "        self.output_intent_dim = output_intent_dim\n",
    "        \n",
    "        # Graph\n",
    "        \n",
    "        # common (contextual feature extraction)\n",
    "        self.bert = BertModel.from_pretrained(self.weights_name)\n",
    "        self.embed_dim = self.bert.embeddings.word_embeddings.embedding_dim \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob) \n",
    "        \n",
    "        # task 1 head\n",
    "        self.iob_classifier_layer = nn.Linear(self.embed_dim, output_iob_dim)\n",
    "        self.init_weights(self.iob_classifier_layer)\n",
    "        \n",
    "        # cascading from task1 (SF) to task2 (ID)\n",
    "        bidirectional = True\n",
    "        self.embed_dim = self.embed_dim\n",
    "        cascading_hidden_dim = 100\n",
    "        self.iob_to_intent_cascading_layer = nn.LSTM(\n",
    "                input_size=self.output_iob_dim,\n",
    "                hidden_size=cascading_hidden_dim,\n",
    "                batch_first=True,\n",
    "                bidirectional=bidirectional)\n",
    "        if bidirectional:\n",
    "            cascading_output_dim = cascading_hidden_dim * 2\n",
    "        else:\n",
    "            cascading_output_dim = cascading_hidden_dim\n",
    "        self.init_weights_lstm(self.iob_to_intent_cascading_layer)             \n",
    "\n",
    "        # task 2 head\n",
    "        intent_input_dim = cascading_output_dim + self.embed_dim\n",
    "        self.intent_classifier_layer = nn.Linear(intent_input_dim, output_intent_dim)\n",
    "        self.init_weights(self.intent_classifier_layer) \n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        torch.nn.init.xavier_uniform(layer.weight) \n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.zero_()\n",
    "\n",
    "    def init_weights_lstm(self, lstm):\n",
    "        for name, param in lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_normal(param)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Here, `X` is a list of two np.array  \n",
    "        consisting of the token_ids (an index into the BERT embedding)\n",
    "        and the attention_mask (a 1 or 0 indicating whether the token \n",
    "        is masked). The `fit` method will \n",
    "        train all these parameters against a softmax objective.\n",
    "        \n",
    "        \"\"\"\n",
    "        # separates the indices from the mask\n",
    "        indices, mask = X\n",
    "        indices = indices.long()\n",
    "        \n",
    "        # graph execution\n",
    "        final_hidden_states, cls_output =\\\n",
    "            self.bert(indices, attention_mask=mask)\n",
    "        \n",
    "        h = self.dropout(final_hidden_states)\n",
    "        \n",
    "        # (batch_size, max_sequence_length, embedding_size)\n",
    "        #  -> (batch_size, max_sequence_length, output_iob_dim)\n",
    "        logits_iob = self.iob_classifier_layer(h)\n",
    "        \n",
    "        # create single embedding per sentence\n",
    "        sentence_h = h.mean(axis=1)\n",
    "        \n",
    "        # cascading: using the iob logits as input of the intent classifier\n",
    "        # (batch_size, max_sequence_length, output_iob_dim)\n",
    "        #  -> (batch_size, output_iob_dim)\n",
    "        lstm_output, (h,c) = self.iob_to_intent_cascading_layer(logits_iob)\n",
    "        # lstm_output: (batch_size, max_sequence_length, cascading_hidden_dim*2)\n",
    "        # (h, c) are cell and output states for each of the intenal layers\n",
    "        # get the output (hidden state) of the first step (bidirectional)\n",
    "        cascade_h = lstm_output.mean(axis=1)\n",
    "        \n",
    "        intent_input = torch.cat((cascade_h, sentence_h), 1)\n",
    "\n",
    "        logits_intent = self.intent_classifier_layer(intent_input)\n",
    "\n",
    "        return logits_iob, logits_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertJointSlu(TorchJointSlu):\n",
    "    def __init__(self, weights_name, *args, **kwargs):\n",
    "        self.weights_name = weights_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.weights_name)\n",
    "        super(BertJointSlu, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def define_graph(self):\n",
    "        \"\"\"This method is used by `fit`. We override it here to use our\n",
    "        new BERT-based graph.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.num_iob_classes = len(self.iob_class2index)   # class2index is set in fit()\n",
    "        self.num_intent_classes = len(self.intent_class2index)   # class2index is set in fit()\n",
    "        print(f\"define_graph: num_iob_classes: {self.num_iob_classes}\")\n",
    "        print(f\"define_graph: num_intent_classes: {self.num_intent_classes}\")\n",
    "        model = BertJointSluModel(\n",
    "            output_iob_dim=self.num_iob_classes, \n",
    "            output_intent_dim=self.num_intent_classes, \n",
    "            dropout_prob=self.hidden_dropout_prob, \n",
    "            weights_name=self.weights_name)\n",
    "        model.train() # flag\n",
    "        return model\n",
    "    \n",
    "    def encode(self, X, max_length=None):\n",
    "        \"\"\"The `X` is a list of strings. We use the model's tokenizer\n",
    "        to get the indices and mask information.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list of [index, mask] pairs, where index is an int and mask\n",
    "        is 0 or 1.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ## IN FINE TUNNIG WE DEAL WITH TOKEN_IDS (indices)\n",
    "        \n",
    "        data = self.tokenizer.batch_encode_plus(\n",
    "            X, \n",
    "            max_length=max_length,\n",
    "            add_special_tokens=True, \n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True)\n",
    "        indices = np.array(data['input_ids'])\n",
    "        mask = np.array(data['attention_mask'])\n",
    "        \n",
    "        return [indices, mask]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a self-contained illustration, starting from the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_for_fine_tunning(hf_fine_tune_mod):\n",
    "    # read ATIS\n",
    "    atis_train = list(atis.train_reader(ATIS_HOME, class_func=atis.slot_filling_and_intent_func))\n",
    "    atis_dev = list(atis.dev_reader(ATIS_HOME, class_func=atis.slot_filling_and_intent_func))\n",
    "    atis_test = list(atis.test_reader(ATIS_HOME, class_func=atis.slot_filling_and_intent_func))\n",
    "\n",
    "    # Split sentence and label\n",
    "    X_atis_sentences_train, y_atis_train = zip(*atis_train)\n",
    "    X_atis_sentences_dev, y_atis_dev = zip(*atis_dev)\n",
    "    X_atis_sentences_test, y_atis_test = zip(*atis_test)\n",
    "\n",
    "    # Split label iob and intent\n",
    "    y_iob_tags_train, y_intent_train = zip(*y_atis_train)\n",
    "    y_iob_tags_dev, y_intent_dev = zip(*y_atis_dev)\n",
    "    y_iob_tags_test, y_intent_test = zip(*y_atis_test)\n",
    "\n",
    "    # align the labels to the sub-word tokens\n",
    "    y_iob_tags_matrix_train, _, _ =  sequence_tagging_label_aligner(X_atis_sentences_train, y_iob_tags_train)\n",
    "    y_iob_tags_matrix_dev, _, _ =  sequence_tagging_label_aligner(X_atis_sentences_dev, y_iob_tags_dev)\n",
    "    y_iob_tags_matrix_test, _, _ =  sequence_tagging_label_aligner(X_atis_sentences_test, y_iob_tags_test)\n",
    "    \n",
    "    # encode the inputs\n",
    "    X_indices_mask_train = hf_fine_tune_mod.encode(X_atis_sentences_train)\n",
    "    X_indices_mask_dev = hf_fine_tune_mod.encode(X_atis_sentences_dev)  \n",
    "    X_indices_mask_test = hf_fine_tune_mod.encode(X_atis_sentences_test)  \n",
    "\n",
    "    # package data\n",
    "    X_train = X_indices_mask_train\n",
    "    X_dev = X_indices_mask_dev\n",
    "    X_test = X_indices_mask_test\n",
    "    \n",
    "    y_train = (y_iob_tags_matrix_train, y_intent_train)\n",
    "    y_dev = (y_iob_tags_matrix_dev, y_intent_dev)\n",
    "    y_test = (y_iob_tags_matrix_test, y_intent_test)\n",
    "    \n",
    "    return X_train, X_dev, X_test, y_train, y_dev, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has some standard fine-tuning parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configures the sequence tagging layer\n",
    "joint_slu_config = {\n",
    "    \"input_embedding_size\": None, # not needed for fine-tunning\n",
    "    \"hidden_dropout_prob\": 0.0,\n",
    "    \"class_weights\": None,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 0.0002,   # eta\n",
    "    \"l2_strength\": 0,\n",
    "    \"max_iter\": 8,   # keep small during debug\n",
    "    \"device\": \"cuda\"\n",
    "}\n",
    "\n",
    "\n",
    "hf_fine_tune_mod = BertJointSlu(\n",
    "    'bert-base-cased', \n",
    "    config=joint_slu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, X_test, y_train, y_dev, y_test =\\\n",
    "    read_dataset_for_fine_tunning(hf_fine_tune_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = hf_fine_tune_mod.fit(X_train, y_train, X_dev=X_dev, y_dev=y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some predictions on the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(X_dev, y_dev):\n",
    "    predict_output = hf_fine_tune_mod.predict(X_dev)\n",
    "\n",
    "    probs_iob = predict_output[\"probs_iob\"]\n",
    "    preds_iob = predict_output[\"preds_iob\"]\n",
    "    attention_mask = predict_output[\"attention_mask\"]\n",
    "\n",
    "    y_iob_tags_matrix_dev, y_intent_dev = y_dev\n",
    "\n",
    "    sequence_tagging_classification_report(y_iob_tags_matrix_dev, (probs_iob, preds_iob, attention_mask))\n",
    "    print(\"SF NON-VERBOSE MACRO-F1:\", sequence_tagging_macro_f1(y_iob_tags_matrix_dev, \n",
    "                                                             (probs_iob, preds_iob, attention_mask)))\n",
    "\n",
    "\n",
    "    print(\"\\n\\n\\nINTENT:\")\n",
    "    print(classification_report(y_intent_dev, predict_output[\"preds_intent\"]))\n",
    "    print(\"INTENT NON-VERBOSE MACRO-F1:\", utils.safe_macro_f1(y_intent_dev,  predict_output[\"preds_intent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
