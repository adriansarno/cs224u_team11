{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU Models -- Intent Determination (ATIS Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Adrian Sarno, Jennifer Arnold\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2020\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all the random seeds for reproducibility. Only the\n",
    "# system and torch seeds are relevant for this notebook.\n",
    "import utils\n",
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6 (default, Jan  8 2020, 13:42:34) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "1.4.0 False\n"
     ]
    }
   ],
   "source": [
    "import sys; print(sys.version)\n",
    "import torch; print(torch.__version__, torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "from torch_rnn_classifier import TorchRNNClassifier, TorchRNNClassifierModel\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.level = logging.ERROR\n",
    "\n",
    "import atis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_weights_name = 'bert-base-cased'\n",
    "\n",
    "hf_tokenizer = BertTokenizer.from_pretrained(hf_weights_name)\n",
    "hf_model = BertModel.from_pretrained(hf_weights_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Example (Wordpiece tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6087, 102] ['[CLS]', '1000', '[SEP]']\n",
      "[101, 122, 117, 1288, 117, 1288, 119, 3135, 102] ['[CLS]', '1', ',', '000', ',', '000', '.', '00', '[SEP]']\n",
      "[101, 1480, 3043, 1121, 8760, 1931, 1106, 1207, 26063, 4661, 102] ['[CLS]', 'night', 'flight', 'from', 'oak', '##land', 'to', 'new', 'yo', '##rk', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "example_texts = [\n",
    "    \"1000\",\n",
    "    \"1,000,000.00\",\n",
    "    \"night flight from oakland to new york\"]\n",
    "\n",
    "for e in example_texts:\n",
    "    ids = hf_tokenizer.encode(e, add_special_tokens=True)\n",
    "    print(ids, hf_tokenizer.convert_ids_to_tokens(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_single_sentence_featurizer(bert_tokenizer, bert_model, sentence):\n",
    "    # we tokenize a single sentence with encode(), so no need for mask\n",
    "    input_ids = bert_tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    X = torch.tensor([input_ids])\n",
    "    with torch.no_grad():\n",
    "        final_hidden_states, cls_output = bert_model(X)\n",
    "        return final_hidden_states.squeeze(0).numpy()    # squeeze batch dimension ( the batch size is 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_single_sentence_featurizer(sentence):\n",
    "    s = sentence\n",
    "    input_ids = hf_tokenizer.encode(s, add_special_tokens=True)\n",
    "    X = torch.tensor([input_ids])\n",
    "    with torch.no_grad():\n",
    "        final_hidden_states, cls_output = hf_model(X)\n",
    "        return final_hidden_states.squeeze(0).numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_single_sentence_featurizer_for_classification(sentence):\n",
    "    reps = bert_single_sentence_featurizer(sentence)\n",
    "    reps = reps.mean(axis=0)\n",
    "    return reps   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching (normalizing sentence lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = np.zeros(shape=4013)\n",
    "for k in range(0, len(input_sentences)-1000, 1000):\n",
    "    batch_sentences = input_sentences[k:k+1000]\n",
    "    print(len(batch_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_featurizer(hf_tokenizer, hf_model, input_sentences):\n",
    "    states_list = []\n",
    "    print(len(input_sentences))\n",
    "    for k in range(0, len(input_sentences)-1000, 1000):\n",
    "        batch_sentences = input_sentences[k:k+1000]\n",
    "        print(len(batch_sentences))\n",
    "        input_token_ids, _, final_hidden_states_avg, _ =\\\n",
    "        bert_batch_featurizer(hf_tokenizer, hf_model, batch_sentences)\n",
    "        states_list.append(final_hidden_states_avg)\n",
    "        print(final_hidden_states_avg.shape)\n",
    "    return torch.cat(states_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_batch_featurizer(hf_tokenizer, hf_model, input_sentences):\n",
    "    tokenizer_output = hf_tokenizer.batch_encode_plus(\n",
    "        input_sentences, \n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True)\n",
    "    \n",
    "    X_input_token_ids = torch.tensor(tokenizer_output['input_ids'])\n",
    "    X_input_mask = torch.tensor(tokenizer_output['attention_mask'])\n",
    "    with torch.no_grad():    \n",
    "        final_hidden_states, cls_output = hf_model(\n",
    "            X_input_token_ids, attention_mask=X_input_mask)        \n",
    "        \n",
    "    # get a sentence representation for the classifier\n",
    "    row_list = []\n",
    "    for row_hidden_states, row_mask in zip(final_hidden_states, X_input_mask):\n",
    "        row_hidden_states_avg = row_hidden_states[row_mask == 1].mean(axis=0)\n",
    "        row_list.append(row_hidden_states_avg)\n",
    "    final_hidden_states_avg = torch.stack(row_list, dim=0)\n",
    "        \n",
    "    return tokenizer_output, final_hidden_states, final_hidden_states_avg, cls_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_ids, final_hidden_states, final_hidden_states_avg, cls_output =\\\n",
    "bert_batch_featurizer(hf_tokenizer, hf_model, example_texts)\n",
    "\n",
    "print(\"tables returned: {},  sentences(rows): {},  tokens(columns): {}\".format(len(input_token_ids), len(input_token_ids['input_ids']), len(input_token_ids['input_ids'][0])))\n",
    "print(input_token_ids.keys())\n",
    "print('input_token_ids:', input_token_ids['input_ids'][0])\n",
    "print('attention_mask:', input_token_ids['attention_mask'][0])\n",
    "final_hidden_states.shape, cls_output.shape\n",
    "\n",
    "print(final_hidden_states_avg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU Models\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent Determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATIS_HOME = os.path.join(\"data\", \"atis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset (already split into train, dev, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "atis_train = list(atis.train_reader(ATIS_HOME, class_func=atis.intent_class_func))\n",
    "atis_dev = list(atis.dev_reader(ATIS_HOME, class_func=atis.intent_class_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"i'm looking for a flight from charlotte to las vegas that stops in st. louis hopefully a dinner flight how can i find that out\",\n",
       " 'atis_flight')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atis_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate sentences and labels into separate lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_atis_sentences_train, y_atis_train = zip(*atis_train)\n",
    "X_atis_sentences_dev, y_atis_dev = zip(*atis_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atis_flight'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_atis_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain a sentence-level representation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time input_token_ids_train, sf_hidden_states_train, final_hidden_states_avg_train, cls_output_train =\\\n",
    "bert_batch_featurizer(hf_tokenizer, hf_model, X_atis_sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time input_token_ids_dev, sf_hidden_states_dev, final_hidden_states_avg_dev, cls_output_dev =\\\n",
    "bert_batch_featurizer(hf_tokenizer, hf_model, X_atis_sentences_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hidden_states_avg_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hidden_states_avg_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a summary of the whole sentence by averaging the sequence of hidden-states for the whole input sequence.\n",
    "\n",
    "    \"\"\"\n",
    "    - cls_output (torch.FloatTensor: of shape (batch_size, hidden_size))\n",
    "    The cls_output variable contains the sentence level embedding returned from the transformer.Model forward method. \n",
    "    This is what the HuggingFace documentation says about it:\n",
    "    Last layer hidden-state of the first token of the sequence (classification token) further processed by a Linear layer and a Tanh activation function. The Linear layer weights are trained \n",
    "    from the next sentence prediction (classification) objective during pre-training.\n",
    "    This output is usually not a good summary of the semantic content of the input, you’re often better with averaging or pooling the sequence of hidden-states for the whole input sequence.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the examples are featurized, we can fit a model and evaluate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_classifier = TorchShallowNeuralClassifier(max_iter=100, hidden_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = id_classifier.fit(X_atis_sentences_train, y_atis_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_preds = id_classifier.predict(X_dev)\n",
    "\n",
    "# sklearn classification report\n",
    "print(classification_report(y_atis_dev, hf_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSTC8\n",
    "id_classifier = TorchShallowNeuralClassifier(max_iter=500, hidden_dim=50)\n",
    "%time _ = id_classifier.fit(final_hidden_states_avg_train, y_dstc8_train[:len(final_hidden_states_avg_train)])\n",
    "hf_preds = id_classifier.predict(final_hidden_states_avg_dev)\n",
    "\n",
    "# sklearn classification report\n",
    "print(classification_report(y_dstc8_dev[:len(final_hidden_states_avg_dev)], hf_preds, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A feed-forward experiment with the ATIS module\n",
    "\n",
    "It is straightforward to conduct experiments like the above using `atis.experiment`, which will enable you to do a wider range of experiments without writing or copy-pasting a lot of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_shallow_network(X, y, **kwargs):\n",
    "    mod = TorchShallowNeuralClassifier(\n",
    "        max_iter=500, hidden_dim=50)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 500 of 500; error is 0.0024766515707597136"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "            atis_abbreviation      1.000     0.882     0.938        17\n",
      "                atis_aircraft      0.769     0.909     0.833        11\n",
      "                 atis_airfare      0.881     0.974     0.925        38\n",
      "atis_airfare#atis_flight_time      0.000     0.000     0.000         1\n",
      "                 atis_airline      0.833     0.833     0.833        18\n",
      "                 atis_airport      1.000     0.333     0.500         3\n",
      "                atis_capacity      0.500     1.000     0.667         1\n",
      "                    atis_city      0.000     0.000     0.000         1\n",
      "                atis_distance      1.000     0.333     0.500         3\n",
      "                  atis_flight      0.978     0.978     0.978       357\n",
      "     atis_flight#atis_airfare      1.000     0.500     0.667         2\n",
      "               atis_flight_no      0.000     0.000     0.000         0\n",
      "             atis_flight_time      0.750     0.667     0.706         9\n",
      "             atis_ground_fare      1.000     0.667     0.800         3\n",
      "          atis_ground_service      1.000     1.000     1.000        25\n",
      "                    atis_meal      0.000     0.000     0.000         0\n",
      "                atis_quantity      1.000     1.000     1.000        10\n",
      "             atis_restriction      0.500     1.000     0.667         1\n",
      "\n",
      "                     accuracy                          0.948       500\n",
      "                    macro avg      0.678     0.615     0.612       500\n",
      "                 weighted avg      0.953     0.948     0.947       500\n",
      "\n",
      "CPU times: user 31min 6s, sys: 39.9 s, total: 31min 46s\n",
      "Wall time: 3min 22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "_ = atis.experiment(\n",
    "    atis_home=ATIS_HOME,\n",
    "    phi=bert_single_sentence_featurizer_for_classification,  # featurizer\n",
    "    batch_phi=None,\n",
    "    label_alignment_func=None,\n",
    "    train_func=fit_shallow_network,\n",
    "    train_reader=atis.train_reader, \n",
    "    assess_reader=atis.dev_reader, \n",
    "    class_func=atis.intent_class_func,\n",
    "    vectorize=False)  # Pass in the BERT hidden state directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT fine-tuning with Hugging Face\n",
    "\n",
    "The above experiments are quite successful – BERT gives us a reliable boost compared to other methods we've explored for the ATIS task. However, we might expect to do even better if we fine-tune the BERT parameters as part of fitting our ATIS classifier. To do that, we need to incorporate the Hugging Face BERT model into our classifier. This too is quite straightforward.\n",
    "\n",
    "The most important step is to create an `nn.Module` subclass that has, for its parameters, both the BERT model and parameters for our own classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HfBertClassifierModel(nn.Module):\n",
    "    def __init__(self, n_classes, weights_name='bert-base-cased'):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.weights_name = weights_name\n",
    "        self.bert = BertModel.from_pretrained(self.weights_name)\n",
    "        self.hidden_dim = self.bert.embeddings.word_embeddings.embedding_dim\n",
    "        # The only new parameters -- the classifier layer:\n",
    "        self.W = nn.Linear(self.hidden_dim, self.n_classes)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Here, `X` is an np.array in which each element is a pair \n",
    "        consisting of an index into the BERT embedding and a 1 or 0\n",
    "        indicating whether the token is masked. The `fit` method will \n",
    "        train all these parameters against a softmax objective.\n",
    "        \n",
    "        \"\"\"\n",
    "        indices = X[: , 0, : ]\n",
    "        # Type conversion, since the base class insists on\n",
    "        # casting this as a FloatTensor, but we ned Long\n",
    "        # for `bert`.\n",
    "        indices = indices.long()\n",
    "        mask = X[: , 1, : ]      \n",
    "        (final_hidden_states, cls_output) = self.bert(\n",
    "            indices, attention_mask=mask)\n",
    "        \n",
    "        # reps = cls_output\n",
    "        reps = final_hidden_states.mean(axis=1)  # for better performance\n",
    "        return self.W(reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training and prediction interface, we can somewhat opportunistically subclass `TorchShallowNeuralClassifier` so that we don't have to write any of our own data-handling, training, or prediction code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HfBertClassifier(TorchShallowNeuralClassifier):\n",
    "    def __init__(self, weights_name, *args, **kwargs):\n",
    "        self.weights_name = weights_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.weights_name)\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def define_graph(self):\n",
    "        \"\"\"This method is used by `fit`. We override it here to use our\n",
    "        new BERT-based graph.\n",
    "        \n",
    "        \"\"\"\n",
    "        bert = HfBertClassifierModel(\n",
    "            self.n_classes_, weights_name=self.weights_name)\n",
    "        \n",
    "        # The following does not train the model\n",
    "        bert.train()   # this just sets a flag that enables modification of pretrained weights\n",
    "        return bert\n",
    "    \n",
    "    def encode(self, X, max_length=None):\n",
    "        \"\"\"The `X` is a list of strings. We use the model's tokenizer\n",
    "        to get the indices and mask information.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list of [index, mask] pairs, where index is an int and mask\n",
    "        is 0 or 1.\n",
    "        \n",
    "        \"\"\"\n",
    "        data = self.tokenizer.batch_encode_plus(\n",
    "            X, \n",
    "            max_length=max_length,\n",
    "            add_special_tokens=True, \n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True)\n",
    "        indices = data['input_ids']\n",
    "        mask = data['attention_mask']\n",
    "        return [[i, m] for i, m in zip(indices, mask)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a self-contained illustration, starting from the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train = list(atis.train_reader(ATIS_HOME, class_func=atis.intent_class_func))\n",
    "hf_dev = list(atis.dev_reader(ATIS_HOME, class_func=atis.intent_class_func))\n",
    "\n",
    "X_hf_sentence_train, y_hf_train = zip(*hf_train)\n",
    "X_hf_sentence_dev, y_hf_dev = zip(*hf_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['show', 'me', 'all', 'round', 'trip', 'flights', 'between', 'houston', 'and', 'las', 'vegas']\n",
      "a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_hf_sentence_dev[1].split())\n",
    "print(y_hf_dev[1][0])\n",
    "\n",
    "len(X_hf_sentence_dev[1].split()), len(y_hf_dev[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has some standard fine-tuning parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_fine_tune_mod = HfBertClassifier(\n",
    "    'bert-base-cased', \n",
    "    batch_size=16, # Crucial; large batches will eat up all your memory!\n",
    "    max_iter=4, \n",
    "    eta=0.00002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can encode them; this step packs together the indices and mask information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hf_indices_train = hf_fine_tune_mod.encode(X_hf_sentence_train)\n",
    "\n",
    "X_hf_indices_dev = hf_fine_tune_mod.encode(X_hf_sentence_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this model is resource intensive. Be patient – it will be worth the wait! (This experiment takes about 10 minutes on a machine with an NVIDIA RTX 2080 Max-Q GPU.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 4 of 4; error is 5.8049565391265792"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7h 35min 27s, sys: 32min 32s, total: 8h 8min\n",
      "Wall time: 1h 7min 50s\n"
     ]
    }
   ],
   "source": [
    "%time _ = hf_fine_tune_mod.fit(X_hf_indices_train, y_hf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some predictions on the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_fine_tune_preds = hf_fine_tune_mod.predict(X_hf_indices_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "            atis_abbreviation      0.941     1.000     0.970        16\n",
      "                atis_aircraft      1.000     0.846     0.917        13\n",
      "                 atis_airfare      1.000     0.905     0.950        42\n",
      "atis_airfare#atis_flight_time      0.000     0.000     0.000         0\n",
      "                 atis_airline      1.000     0.947     0.973        19\n",
      "                 atis_airport      0.667     1.000     0.800         2\n",
      "                atis_capacity      1.000     1.000     1.000         1\n",
      "                    atis_city      1.000     1.000     1.000         1\n",
      "                atis_distance      1.000     1.000     1.000         3\n",
      "                  atis_flight      0.980     0.997     0.989       351\n",
      "     atis_flight#atis_airfare      1.000     0.667     0.800         3\n",
      "             atis_flight_time      1.000     0.900     0.947        10\n",
      "             atis_ground_fare      1.000     1.000     1.000         3\n",
      "          atis_ground_service      1.000     1.000     1.000        25\n",
      "                atis_quantity      1.000     1.000     1.000        10\n",
      "             atis_restriction      1.000     1.000     1.000         1\n",
      "\n",
      "                     accuracy                          0.980       500\n",
      "                    macro avg      0.912     0.891     0.897       500\n",
      "                 weighted avg      0.983     0.980     0.981       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(hf_fine_tune_preds, y_hf_dev, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is just one of the many possible ways to fine-tune BERT using our course modules or new modules you write. The crux of it is creating an `nn.Module` that combines the BERT parameters with your model's new parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
