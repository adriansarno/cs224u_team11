{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU Models Slot Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Adrian Sarno, Jennifer Arnold\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2020\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all the random seeds for reproducibility. Only the\n",
    "# system and torch seeds are relevant for this notebook.\n",
    "import utils\n",
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.level = logging.ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace import\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import atis\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA test\n",
    "import sys; print(sys.version)\n",
    "import torch; print(torch.__version__, torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hf_weights_name = 'bert-base-cased'\n",
    "# hf_weights_name = 'bert-base-uncased' - in this case the tokenizer does not split into subwords so often\n",
    "\n",
    "hf_tokenizer = BertTokenizer.from_pretrained(hf_weights_name)\n",
    "hf_model = BertModel.from_pretrained(hf_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATIS_HOME = os.path.join(\"data\", \"atis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching (normalizing sentence lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encoder_vectorizer(input_sentences, max_length=None):\n",
    "    \"\"\"\n",
    "    This function accomplishes two tasks:\n",
    "    1.  tokenization and sentence-length normalization\n",
    "    2.  featurization, it calls the bert model to convert tokens to embeddings \n",
    "    \"\"\"\n",
    "    \n",
    "    # tokenization, encoding and sentence-length normalization\n",
    "    tokenizer_output = hf_tokenizer.batch_encode_plus(\n",
    "        input_sentences, \n",
    "        max_length=max_length,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True)\n",
    "    \n",
    "    input_token_ids = torch.tensor(tokenizer_output['input_ids'])\n",
    "    input_mask = torch.tensor(tokenizer_output['attention_mask'])\n",
    "\n",
    "    # featurization\n",
    "    with torch.no_grad():\n",
    "        final_hidden_states, cls_output = \\\n",
    "        hf_model(input_token_ids, attention_mask=input_mask)\n",
    "    \n",
    "    # cls_output not used\n",
    "    # convert to numpy to match the type of all other results (all numpy)\n",
    "    final_hidden_states = final_hidden_states.detach().cpu().numpy()\n",
    "    \n",
    "    return final_hidden_states, np.array(tokenizer_output['attention_mask']), np.array(tokenizer_output['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchShallowClassifierModel(nn.Module):\n",
    "    def __init__(self,\n",
    "            embed_dim,\n",
    "            output_dim,\n",
    "            dropout_prob):\n",
    "        super(TorchShallowClassifierModel, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Graph\n",
    "        self.dropout = nn.Dropout(dropout_prob)  \n",
    "        self.classifier_layer = nn.Linear(embed_dim, output_dim)\n",
    "        \n",
    "        torch.nn.init.xavier_uniform(self.classifier_layer.weight) \n",
    "        if self.classifier_layer.bias is not None:\n",
    "            self.classifier_layer.bias.data.zero_()\n",
    "\n",
    "            \n",
    "    def forward(self, X):\n",
    "    \n",
    "        # separate the feature vectors (embeddings) from the attention_mask\n",
    "        X, attention_mask = X\n",
    "\n",
    "        X = self.dropout(X)\n",
    "        logits = self.classifier_layer(X)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_model_base import TorchModelBase\n",
    "from utils import progress_bar\n",
    "\n",
    "class TorchShallowSequenceTagger(TorchModelBase):\n",
    "    \"\"\"\n",
    "     Featurization:\n",
    "        Takes the embeddings already pre-computed.\n",
    "    \n",
    "    Classification:\n",
    "        The simplest token classifier uses just a linear layer.\n",
    "        The Pytorch linear layer can take as input a tensor of any number of dimensions\n",
    "        and only the last dimension needs to be specified as input dimension.\n",
    "   \n",
    "        https://pytorch.org/docs/master/nn.html#linear   \n",
    "    \"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super(TorchShallowSequenceTagger, self).__init__(**kwargs)\n",
    "        \n",
    "        self.config = config\n",
    "        self.input_embedding_size = config[\"input_embedding_size\"]\n",
    "        self.hidden_dropout_prob = config[\"hidden_dropout_prob\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.l2_strength = config[\"l2_strength\"]\n",
    "        self.max_iter = config[\"max_iter\"]\n",
    "        self.device = config[\"device\"]\n",
    "        self.class_weights = config.get(\"class_weights\", None)\n",
    "        if self.class_weights is not None:\n",
    "            class_weights = torch.FloatTensor(self.class_weights)\n",
    "        \n",
    "    def define_graph(self):\n",
    "        \"\"\"\n",
    "        This is a shallow model. so it does not really define a graph here \n",
    "        but it instantiates a model class with the classfier top.\n",
    "        \"\"\"     \n",
    "        self.num_classes = len(self.class2index)   # class2index is set in fit()\n",
    "        print(f\"define_graph: num_classes: {self.num_classes}\")\n",
    "        return TorchShallowClassifierModel(\n",
    "            self.input_embedding_size, \n",
    "            self.num_classes, \n",
    "            self.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, X=None):\n",
    "        \"\"\"\n",
    "        X: token embeddings\n",
    "        \n",
    "        attention_mask:\n",
    "            This argument is only needed if you want to compute the training loss\n",
    "            \n",
    "        labels: \n",
    "            2D tensor (batch_size, max_sequence_length), \n",
    "            each element is a class index in the range [0, C-1], no one-hot encodding required.\n",
    "            In mini-batch processing mode, the labels vectors must be padded \n",
    "            up to max_sequence_length.\n",
    "            This argument is only needed if you want to compute the training loss\n",
    "        \"\"\"\n",
    "        \n",
    "        # call the forward method on the classification model\n",
    "        logits = self.model.forward(X=X)                \n",
    "        outputs = (logits,) \n",
    "\n",
    "\n",
    "        return outputs  # (loss), logits\n",
    "\n",
    "    def compute_loss(self, logits, attention_mask, labels):\n",
    "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "        active_logits = logits.view(-1, self.num_classes)\n",
    "\n",
    "        if attention_mask is not None:              \n",
    "            # computes a boolean mask (flat boolean array), one element for each token of each row\n",
    "            active_loss_mask = attention_mask.view(-1) == 1\n",
    "            active_labels = torch.where(\n",
    "                active_loss_mask, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "            )\n",
    "        else:\n",
    "            active_labels = labels.view(-1)\n",
    "\n",
    "        # computes the loss between labels and logits \n",
    "        loss = loss_fct(active_logits, active_labels)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\"Standard `fit` method.\n",
    "        \n",
    "        fit() expects embeddings in X and strings in y.\n",
    "        The class itself is in charge of encoding the labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : [embeddings, attention_mask]\n",
    "        y : array-like, a list of lists of string [['O', 'B-fromcity']]\n",
    "        kwargs : dict\n",
    "            For passing other parameters. If 'X_dev' is included,\n",
    "            then performance is monitored every 10 epochs; use\n",
    "            `dev_iter` to control this number.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "\n",
    "        \"\"\"               \n",
    "        \n",
    "        ################################################################\n",
    "        # Model definition\n",
    "        ################################################################\n",
    "        \n",
    "        # Graph:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            \n",
    "            self.compute_class2index(y)  # expects strings, must run before tensorizing y\n",
    "            self.model = self.define_graph()\n",
    "        \n",
    "        # Prime the model for training\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        \n",
    "        # Default is torch.optim.Adam\n",
    "        optimizer = self.optimizer(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.l2_strength)    \n",
    "          \n",
    "        \n",
    "        ################################################################\n",
    "        # Data \n",
    "        ################################################################\n",
    "        # separate the feature vectors (embeddings) from the attention_mask\n",
    "        X, attention_mask = X\n",
    "        print(f\"X: {type(X)},     attention_mask: {type(attention_mask)},    y: [{len(y)}, {len(y[0])}]\")\n",
    "         \n",
    "        # Compute Incremental performance:\n",
    "        X_dev = kwargs.get('X_dev')\n",
    "        if X_dev is not None:\n",
    "            # X_dev contains 2 parameters, X_dev, attention_mask_dev\n",
    "            # the dev mask stays as numpy as is not used for anything\n",
    "            # because the class does not compute dev loss, it just stores \n",
    "            # the dev predictions. This mask is required by predict but\n",
    "            # it is just returned in the predict results (never used)            \n",
    "            dev_iter = kwargs.get('dev_iter', 10)\n",
    "        \n",
    "        # encode labels (label vectorization). must run before tensorizing y\n",
    "        y = self.encode_labels(y)\n",
    "        y = self.pad_to_max_length(y, X.shape[:2]) # y must have the shape of first 2 dims of X\n",
    "\n",
    "        # cast data into PyTorch tensors\n",
    "        X = torch.FloatTensor(X)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.bool)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        # Wrap data into a dataset and use a Dataloader for batching\n",
    "        dataset = torch.utils.data.TensorDataset(X, attention_mask, y)\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, \n",
    "            shuffle=True, pin_memory=True) \n",
    "        \n",
    "\n",
    "        ################################################################\n",
    "        # Training process (Gradient Descent)\n",
    "        ################################################################\n",
    "        for iteration in range(1, self.config[\"max_iter\"]+1):\n",
    "            epoch_error = 0.0\n",
    "            for i, (X_batch, m_batch, y_batch) in enumerate(dataloader):\n",
    "                \n",
    "                # load the batch input tensors into GPU memory\n",
    "                X_batch = X_batch.to(self.device, non_blocking=True)\n",
    "                m_batch = m_batch.to(self.device, non_blocking=True)\n",
    "                \n",
    "                # call forward (mask is not used unless you want to compute the training loss)\n",
    "                logits = self.model.forward(\n",
    "                    X=[X_batch, m_batch])\n",
    "                \n",
    "                # load the batch label tensors into GPU memory\n",
    "                y_batch = y_batch.to(self.device, non_blocking=True)\n",
    "                \n",
    "                # compute the loss, the gradients and update the weights\n",
    "                err = self.compute_loss(logits, m_batch, y_batch)\n",
    "                epoch_error += err.item()\n",
    "                optimizer.zero_grad()\n",
    "                err.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Incremental predictions where possible:\n",
    "            if X_dev is not None and iteration > 0 and iteration % dev_iter == 0:\n",
    "                self.dev_predictions[iteration] = self.predict(X_dev)\n",
    "                self.model.train()\n",
    "            self.errors.append(epoch_error)\n",
    "            progress_bar(\n",
    "                \"Finished epoch {} of {}; error is {}\".format(\n",
    "                    iteration, self.config[\"max_iter\"], epoch_error))\n",
    "        return self\n",
    "\n",
    "\n",
    "    \n",
    "    def predict_flat(self, X):\n",
    "        \"\"\"Predicted classes for the examples in `X`. In flat format for metric functions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "        attention_mask: input mask\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probs: torch.tensor(batch_size * max_sequence_length, num_classes)\n",
    "        pred_class: numpy.array(batch_size * max_sequence_length)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        probs, preds, attention_mask = self.predict(X)\n",
    "        \n",
    "        # Flatten and apply mask\n",
    "        preds_flat = preds.flatten()\n",
    "        preds_flat = preds_flat[attention_mask.flatten() == 1]        \n",
    "        \n",
    "        probs_flat = probs.flatten()\n",
    "        probs_flat = probs_flat[attention_mask.flatten() == 1] \n",
    "        \n",
    "        return probs_flat, preds_flat\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicted classes for the examples in `X`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "        attention_mask: np.array \n",
    "            input mask\n",
    "            # the dev mask stays as numpy as is not used for anything\n",
    "            # because the class does not compute dev loss, it just stores \n",
    "            # the dev predictions. This mask is required by predict but\n",
    "            # it is just returned in the predict results (never used)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probs: torch.tensor(batch_size, max_sequence_length, num_classes)\n",
    "        pred_class: numpy.array(batch_size, max_sequence_length)\n",
    "        attention_mask\n",
    "\n",
    "        \"\"\"\n",
    "        # get the attention_mask to return it\n",
    "        _, attention_mask = X\n",
    "\n",
    "        # compute probabilities and predicted class\n",
    "        probs = self.predict_proba(X)\n",
    "\n",
    "        # compute predicted class, maximizing across the last dimension (classes)\n",
    "        _, pred_class_idx = torch.max(probs, dim=-1)\n",
    "        pred_class_idx = pred_class_idx.detach().cpu().numpy()\n",
    "        \n",
    "        # decode the class indices tocla class names (IOB_tag)\n",
    "        preds = [[self.index2class[i] for i in row_class_idx] \n",
    "                              for row_class_idx in pred_class_idx]\n",
    "        preds = np.array(preds)\n",
    "\n",
    "        return probs.detach().cpu().numpy(), preds, attention_mask\n",
    "        \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predicted probabilities for the examples in `X`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor(batch_size, max_sequence_length, num_classes)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Graph:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            # self.class2index must be defined in this case\n",
    "            self.model = self.define_graph()\n",
    "        \n",
    "        # prime the model for prediction-only mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            self.model.to(self.device)\n",
    "       \n",
    "            # cast input data into PyTorch tensors\n",
    "            x, attention_mask = X\n",
    "            x, attention_mask = torch.tensor(x), torch.tensor(attention_mask, dtype=torch.bool)\n",
    "\n",
    "            # load the input tensors into GPU memory\n",
    "            x = x.to(self.device)\n",
    "            attention_mask = attention_mask.to(self.device)\n",
    "            \n",
    "            # call forward \n",
    "            output = self.forward(\n",
    "                X=[x, attention_mask])\n",
    "            \n",
    "            logits = output[0]\n",
    "            \n",
    "            # compute probabilities and predicted class\n",
    "            probs = nn.Softmax(dim=-1)(logits) # normalize scores along the latest dimension\n",
    "            \n",
    "            return probs  # tensor (no_grad)\n",
    "        \n",
    "    def compute_class2index(self, y):\n",
    "        \"\"\"\n",
    "        y: 2-D list of (lists of) strings (iob_tags, not indices)\n",
    "        \n",
    "        expects strings, must run before tensorizing y,\n",
    "        must run before defining the graph because it computes\n",
    "        the output network output (num_classes)\n",
    "        \n",
    "        Note:\n",
    "        if the input type is incorrect and it results in a number of classes is incorrect (very high) \n",
    "        it will likely cause a CUDA-OUT-OF-MEMORY error\n",
    "        \"\"\"\n",
    "\n",
    "        # flat list of iob labels\n",
    "        iob_labels = []\n",
    "        for y_row in y:\n",
    "            iob_labels.extend(y_row)\n",
    "            \n",
    "        # create mapping\n",
    "        classes = sorted(set(iob_labels))\n",
    "        self.class2index = dict(zip(classes, range(len(classes))))\n",
    "        self.index2class = {i:c for c, i in self.class2index.items()}        \n",
    "        \n",
    "    def encode_labels(self, y):\n",
    "        \"\"\"\n",
    "        y: 2-D list of (lists of) strings (iob_tags, not indices)\n",
    "        \n",
    "        expects strings, must run before tensorizing y,\n",
    "        \"\"\"\n",
    "        tag_id_matrix = []\n",
    "        for iob_tags in y:\n",
    "            tag_ids = [self.class2index[iob_tag] for iob_tag in iob_tags]\n",
    "            tag_id_matrix.append(tag_ids)  \n",
    "\n",
    "        return  tag_id_matrix\n",
    "    \n",
    "    def pad_to_max_length(self, jagged_matrix, output_shape):\n",
    "        padded_matrix = np.zeros(shape=output_shape)\n",
    "        for i, row in enumerate(jagged_matrix):\n",
    "            padded_matrix[i, :len(row)] = row \n",
    "        return padded_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Training and Prediction of the Shallow Slot Filling model (the Shallow model uses Bert embeddings without fine-tunning)\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling: Alignment, Encoding, Normalize the length of the sequences, Class Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(class_ids):\n",
    "    \"\"\"\n",
    "        class_ids: 1D tensor, contains one class_id for each example\n",
    "    \"\"\"\n",
    "    # encode the class_ids as onehot\n",
    "    class_matrix = np.zeros(shape=(len(class_ids), max(class_ids)))\n",
    "    class_matrix[class_ids] = 1\n",
    "    \n",
    "    # set the positive weights as the fraction of negative labels (0) for each class (each column)\n",
    "    w_p = np.sum(class_matrix == 0, axis=0) / class_matrix.shape[0]\n",
    "\n",
    "    # set the negative weights as the fraction of positive labels (1) for each class (each column)\n",
    "    w_n = np.sum(class_matrix == 1, axis=0) / class_matrix.shape[0]\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label and Sub-token Alignment (call WordPiece for each token, output word_to_tok_map and aligned_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_start_label_aligner(sentence, word_labels=None):\n",
    "    \"\"\"\n",
    "    Aligns the IOB labels to the word-starting tokens in the list of\n",
    "    sub-word tokens returned by the WordPiece tokenizer.\n",
    "    Returns:\n",
    "    - an array of indices, each pointing to the first sub-token of every word\n",
    "    - a padded list of labels, which has one element for each sub-token (the first\n",
    "      sub-token of every word gets the label, the rest get the padding label 'X')\n",
    "    \"\"\"\n",
    "    # Token map will be an int -> int mapping between the `word` index in the sentence and\n",
    "    # the WordPiece `tokens` index.\n",
    "    word_start_indices = []\n",
    "    tokens = [\"[CLS]\"]\n",
    "    if word_labels is not None:\n",
    "        token_labels = [\"O\"]\n",
    "    else:\n",
    "        token_labels = None\n",
    "    if len(sentence.split()) != len(word_labels):\n",
    "        print(f\"sentence: {len(sentence.split())}, word_labels: {len(word_labels)}\")\n",
    "        print(f\"sentence: {sentence.split()}, word_labels: {word_labels}\")\n",
    "    for word_idx, word in enumerate(sentence.split(' ')):\n",
    "        word_start_indices.append(len(tokens))\n",
    "        word_tokens = hf_tokenizer.tokenize(word)  # tokenize ONE word \n",
    "        tokens.extend(word_tokens)\n",
    "        if word_labels is not None:\n",
    "            token_labels.append(word_labels[word_idx])\n",
    "            if len(word_tokens) > 1:\n",
    "                token_labels.extend([\"X\"]*(len(word_tokens)-1))\n",
    "\n",
    "    tokens.append(\"[SEP]\")\n",
    "    token_labels.append( \"O\")\n",
    "    return token_labels, word_start_indices, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_label_aligner(sentences, labels):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    label_matrix, word_start_matrix, token_matrix = [], [], []\n",
    "    \n",
    "    for sentence, word_labels in zip(sentences, labels):\n",
    "        token_labels, word_start_indices, tokens =\\\n",
    "            word_start_label_aligner(sentence, word_labels)\n",
    "        \n",
    "        label_matrix.append(token_labels)\n",
    "        word_start_matrix.append(word_start_indices)\n",
    "        token_matrix.append(tokens)\n",
    "\n",
    "    return label_matrix, word_start_matrix, token_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_predict_output(y_true, preds, attention_mask):\n",
    "    \"\"\"\n",
    "    y_true : list of list of strings (IOB_tags)\n",
    "        y_true is a list of  variable-length lists of strings, \n",
    "        is token-alignmed but not lenght-padded\n",
    "        \n",
    "    preds : np.array(batch_size, max_sentence_length)\n",
    "        2-D array with the class predicted by the model,\n",
    "        for each token of each sentence.\n",
    "        The attention mask must be applied to exclude the padding tokens.\n",
    "        \n",
    "    attention_mask: np.array(batch_size, max_sentence_length)\n",
    "        boolean tensor to filter the padding tokens\n",
    "    \n",
    "    In order to produce a classification report for sequence tagging, \n",
    "    first al the arrays need to be flattened.\n",
    "    \"\"\"\n",
    "\n",
    "    # flatten the sequence labels\n",
    "    y_flat = []\n",
    "    for iob_tags in y_true:\n",
    "        y_flat.extend(iob_tags)\n",
    "\n",
    "    # apply mask to remove padding token positions and flatten the matrix\n",
    "    preds_flat = preds.flatten()\n",
    "    preds_flat = preds_flat[attention_mask.flatten() == 1]\n",
    "    \n",
    "    return y_flat, preds_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_classification_report(y, predict_output, digits=3):\n",
    "    \"\"\"\n",
    "    Adapts the interface between the experiment and the sequence-tagging report function\n",
    "    y : non-padded token_label_matrix\n",
    "        list of list of strings (IOB_tags)\n",
    "        y is a list of  variable-length lists of strings, \n",
    "        is token-alignmed but not lenght-padded\n",
    "        \n",
    "    preds : np.array(batch_size, max_sentence_length)\n",
    "        2-D array with the class predicted by the model,\n",
    "        for each token of each sentence.\n",
    "        The attention mask must be applied to exclude the padding tokens.\n",
    "        \n",
    "    attention_mask:\n",
    "        boolean tensor to filter the padding tokens\n",
    "    \"\"\"\n",
    "    probs, preds, attention_mask = predict_output[:3]\n",
    "        \n",
    "    y_flat, preds_flat =\\\n",
    "        flatten_predict_output(y, preds, attention_mask)\n",
    "\n",
    "    print(classification_report(y_flat, preds_flat, digits=digits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_macro_f1(y, predict_output, digits=3):\n",
    "    \"\"\"\n",
    "    Adapts the interface between the experiment and the sequence-tagging scoring function\n",
    "    y : non-padded token_label_matrix\n",
    "        list of list of strings (IOB_tags)\n",
    "        y is a list of  variable-length lists of strings, \n",
    "        is token-alignmed but not lenght-padded\n",
    "        \n",
    "        \n",
    "    preds : np.array(batch_size, max_sentence_length)\n",
    "        2-D array with the class predicted by the model,\n",
    "        for each token of each sentence.\n",
    "        The attention mask must be applied to exclude the padding tokens.\n",
    "        \n",
    "    attention_mask:\n",
    "        boolean tensor to filter the padding tokens\n",
    "    \"\"\"\n",
    "    probs, preds, attention_mask = predict_output[:3]\n",
    "    \n",
    "    y_flat, pred_flat =\\\n",
    "        flatten_predict_output(y, preds, attention_mask)\n",
    "    \n",
    "    return utils.safe_macro_f1(y_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A feed-forward experiment with the ATIS module\n",
    "* * *\n",
    "\n",
    "It is straightforward to conduct experiments like the above using `atis.experiment`, which will enable you to do a wider range of experiments without writing or copy-pasting a lot of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_fit(X,  y):\n",
    "    \"\"\"\n",
    "        X : contains the embeddings and the attention_mask\n",
    "            X[0]: embeddings torchTensor([batch_size, max_sentence_length, 768])\n",
    "            fit() expects a tensor of embeddings\n",
    "        \n",
    "            X[1]: attention_mask: torchTensor([batch_size, max_sentence_length])\n",
    "            expects the tensor input_mask_train\n",
    "            it is used to mask the padding tokens in X\n",
    "            each value is 0 or 1.\n",
    "\n",
    "        y: torchTensor([batch_size, max_sentence_length])\n",
    "            expects the tensor padded_token_label_matrix_train\n",
    "            each value is a class string (IOB_tag)\n",
    "    \"\"\"        \n",
    "\n",
    "    # configures the sequence tagging layer\n",
    "    sequence_tagging_config = {\n",
    "        \"input_embedding_size\": 768,\n",
    "        \"hidden_dropout_prob\": 0.4,\n",
    "        \"class_weights\": None,\n",
    "        \"batch_size\": 64,\n",
    "        \"lr\": 1e-3,\n",
    "        \"l2_strength\": 0,\n",
    "        \"max_iter\": 50,   # keep small during debug\n",
    "        \"device\": \"cuda\"\n",
    "    }\n",
    "    \n",
    "    # instantiates the network\n",
    "    shallow_sf = TorchShallowSequenceTagger(sequence_tagging_config)\n",
    "\n",
    "    # Fit\n",
    "    return shallow_sf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_phi(sentences):\n",
    "    \"\"\"\n",
    "    transformer-based batch tokenzer, encoder, vectorizer, and sequence padding\n",
    "    \"\"\"\n",
    "\n",
    "    final_hidden_states, attention_mask, input_token_ids =\\\n",
    "        batch_encoder_vectorizer(sentences)\n",
    "    \n",
    "    return final_hidden_states, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "exp_results = atis.experiment(\n",
    "    ATIS_HOME,\n",
    "    phi=None,\n",
    "    batch_phi=sequence_tagging_phi,\n",
    "    label_alignment_func=sequence_tagging_label_aligner,\n",
    "    train_func=sequence_tagging_fit,\n",
    "    train_reader=atis.train_reader, \n",
    "    assess_reader=atis.dev_reader, \n",
    "    class_func=atis.slot_filling_func,   # label selector\n",
    "    metrics_report_func=sequence_tagging_classification_report,\n",
    "    score_func=sequence_tagging_macro_f1,\n",
    "    vectorize=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSequenceTaggingModel(nn.Module):\n",
    "    def __init__(self,\n",
    "            output_dim,\n",
    "            hidden_dropout_prob,\n",
    "            weights_name='bert-base-cased'):\n",
    "        super(BertSequenceTaggingModel, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.weights_name = weights_name\n",
    "        \n",
    "        # Graph\n",
    "        self.bert = BertModel.from_pretrained(self.weights_name)\n",
    "        self.embed_dim = self.bert.embeddings.word_embeddings.embedding_dim \n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)  \n",
    "        self.classifier_layer = nn.Linear(self.embed_dim, output_dim)\n",
    "        \n",
    "        # init weights\n",
    "        torch.nn.init.xavier_uniform(self.classifier_layer.weight) \n",
    "        if self.classifier_layer.bias is not None:\n",
    "            self.classifier_layer.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Here, `X` is a list of two np.array  \n",
    "        consisting of the token_ids (an index into the BERT embedding)\n",
    "        and the attention_mask (a 1 or 0 indicating whether the token \n",
    "        is masked). The `fit` method will \n",
    "        train all these parameters against a softmax objective.\n",
    "        \n",
    "        \"\"\"\n",
    "        # separates the indices from the mask\n",
    "        indices, mask = X\n",
    "        # Type conversion, since the base class insists on\n",
    "        # casting this as a FloatTensor, but we ned Long\n",
    "        # for `bert`.\n",
    "        indices = indices.long()\n",
    "        \n",
    "        # graph execution\n",
    "        final_hidden_states, cls_output =\\\n",
    "            self.bert(indices, attention_mask=mask)\n",
    "        \n",
    "        h = self.dropout(final_hidden_states)\n",
    "        \n",
    "        logits = self.classifier_layer(h)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSequenceTagging(TorchShallowSequenceTagger):\n",
    "    def __init__(self, weights_name, *args, **kwargs):\n",
    "        self.weights_name = weights_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.weights_name)\n",
    "        super(BertSequenceTagging, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def define_graph(self):\n",
    "        \"\"\"This method is used by `fit`. We override it here to use our\n",
    "        new BERT-based graph.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.num_classes = len(self.class2index)   # class2index is set in fit()\n",
    "        model = BertSequenceTaggingModel(\n",
    "            output_dim=self.num_classes,\n",
    "            hidden_dropout_prob=self.hidden_dropout_prob, \n",
    "            weights_name=self.weights_name)\n",
    "        model.train() # flag\n",
    "        return model\n",
    "    \n",
    "    def encode(self, X, max_length=None):\n",
    "        \"\"\"The `X` is a list of strings. We use the model's tokenizer\n",
    "        to get the indices and mask information.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list of [index, mask] pairs, where index is an int and mask\n",
    "        is 0 or 1.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ## IN FINE TUNNIG WE DEAL WITH TOKEN_IDS (indices)\n",
    "        \n",
    "        data = self.tokenizer.batch_encode_plus(\n",
    "            X, \n",
    "            max_length=max_length,\n",
    "            add_special_tokens=True, \n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True)\n",
    "        indices = np.array(data['input_ids'])\n",
    "        mask = np.array(data['attention_mask'])\n",
    "        \n",
    "        return [indices, mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a self-contained illustration, starting from the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train = list(atis.train_reader(ATIS_HOME, class_func=atis.slot_filling_func))\n",
    "hf_dev = list(atis.dev_reader(ATIS_HOME, class_func=atis.slot_filling_func))\n",
    "\n",
    "X_hf_sentence_train, y_train = zip(*hf_train)\n",
    "X_hf_sentence_dev, y_dev = zip(*hf_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has some standard fine-tuning parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configures the sequence tagging layer\n",
    "sequence_tagging_config = {\n",
    "    \"input_embedding_size\": None, # not needed for fine-tunning\n",
    "    \"hidden_dropout_prob\": 0.4,\n",
    "    \"class_weights\": None,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 0.0002,   # eta\n",
    "    \"l2_strength\": 0,\n",
    "    \"max_iter\": 4,   # keep small during debug\n",
    "    \"device\": \"cuda\"\n",
    "}\n",
    "\n",
    "\n",
    "hf_fine_tune_mod = BertSequenceTagging(\n",
    "    'bert-base-cased', \n",
    "    config=sequence_tagging_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can encode them; this step packs together the indices and mask information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_indices_mask_train = hf_fine_tune_mod.encode(X_hf_sentence_train)\n",
    "\n",
    "X_indices_mask_dev = hf_fine_tune_mod.encode(X_hf_sentence_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = [np.ones(shape=(2, 2, 5)), np.zeros(shape=(2, 2))]\n",
    "x, m = X\n",
    "X = [torch.FloatTensor(x),torch.tensor(m, dtype=torch.bool)]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this model is resource intensive. Be patient â€“ it will be worth the wait! (This experiment takes about 10 minutes on a machine with an NVIDIA RTX 2080 Max-Q GPU.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, _, _ =  sequence_tagging_label_aligner(X_hf_sentence_train, y_train)\n",
    "\n",
    "y_dev, _, _ =  sequence_tagging_label_aligner(X_hf_sentence_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = hf_fine_tune_mod.fit(X_indices_mask_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some predictions on the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_fine_tune_preds = hf_fine_tune_mod.predict(X_indices_mask_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequence_tagging_classification_report(y_dev, hf_fine_tune_preds, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
