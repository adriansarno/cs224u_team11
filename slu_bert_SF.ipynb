{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU Models Slot Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Adrian Sarno, Jennifer Arnold\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2020\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all the random seeds for reproducibility. Only the\n",
    "# system and torch seeds are relevant for this notebook.\n",
    "import utils\n",
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.level = logging.ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace import\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import atis\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "from torch_rnn_classifier import TorchRNNClassifier, TorchRNNClassifierModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n",
      "1.4.0 True\n",
      "GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "# CUDA test\n",
    "import sys; print(sys.version)\n",
    "import torch; print(torch.__version__, torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hf_weights_name = 'bert-base-cased'\n",
    "# hf_weights_name = 'bert-base-uncased' - in this case the tokenizer does not split into subwords so often\n",
    "\n",
    "hf_tokenizer = BertTokenizer.from_pretrained(hf_weights_name)\n",
    "hf_model = BertModel.from_pretrained(hf_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATIS_HOME = os.path.join(\"data\", \"atis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching (normalizing sentence lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encoder_vectorizer(input_sentences, max_length=None):\n",
    "    \"\"\"\n",
    "    This function accomplishes two tasks:\n",
    "    1.  tokenization and sentence-length normalization\n",
    "    2.  featurization, it calls the bert model to convert tokens to embeddings \n",
    "    \"\"\"\n",
    "    \n",
    "    # tokenization, encoding and sentence-length normalization\n",
    "    tokenizer_output = hf_tokenizer.batch_encode_plus(\n",
    "        input_sentences, \n",
    "        max_length=max_length,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True)\n",
    "    \n",
    "    input_token_ids = torch.tensor(tokenizer_output['input_ids'])\n",
    "    input_mask = torch.tensor(tokenizer_output['attention_mask'])\n",
    "\n",
    "    # featurization\n",
    "    with torch.no_grad():\n",
    "        final_hidden_states, cls_output = \\\n",
    "        hf_model(input_token_ids, attention_mask=input_mask)\n",
    "    \n",
    "    # cls_output not used\n",
    "    # convert to numpy to match the type of all other results (all numpy)\n",
    "    final_hidden_states = final_hidden_states.detach().cpu().numpy()\n",
    "    \n",
    "    return final_hidden_states, np.array(tokenizer_output['attention_mask']), np.array(tokenizer_output['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchShallowClassifierModel(nn.Module):\n",
    "    def __init__(self,\n",
    "            embed_dim,\n",
    "            output_dim,\n",
    "            dropout_prob):\n",
    "        super(TorchShallowClassifierModel, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Graph\n",
    "        self.dropout = nn.Dropout(dropout_prob)  \n",
    "        self.classifier_layer = nn.Linear(embed_dim, output_dim)\n",
    "        \n",
    "        torch.nn.init.xavier_uniform(self.classifier_layer.weight) \n",
    "        if self.classifier_layer.bias is not None:\n",
    "            self.classifier_layer.bias.data.zero_()\n",
    "\n",
    "            \n",
    "    def forward(self, X):\n",
    "    \n",
    "        # separate the feature vectors (embeddings) from the attention_mask\n",
    "        X, attention_mask = X\n",
    "\n",
    "        X = self.dropout(X)\n",
    "        logits = self.classifier_layer(X)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_model_base import TorchModelBase\n",
    "from utils import progress_bar\n",
    "\n",
    "class TorchShallowSequenceTagger(TorchModelBase):\n",
    "    \"\"\"\n",
    "     Featurization:\n",
    "        Takes the embeddings already pre-computed.\n",
    "    \n",
    "    Classification:\n",
    "        The simplest token classifier uses just a linear layer.\n",
    "        The Pytorch linear layer can take as input a tensor of any number of dimensions\n",
    "        and only the last dimension needs to be specified as input dimension.\n",
    "   \n",
    "        https://pytorch.org/docs/master/nn.html#linear   \n",
    "    \"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super(TorchShallowSequenceTagger, self).__init__(**kwargs)\n",
    "        \n",
    "        self.config = config\n",
    "        self.input_embedding_size = config[\"input_embedding_size\"]\n",
    "        self.hidden_dropout_prob = config[\"hidden_dropout_prob\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.l2_strength = config[\"l2_strength\"]\n",
    "        self.max_iter = config[\"max_iter\"]\n",
    "        self.device = config[\"device\"]\n",
    "        self.class_weights = config.get(\"class_weights\", None)\n",
    "        if self.class_weights is not None:\n",
    "            class_weights = torch.FloatTensor(self.class_weights)\n",
    "        \n",
    "    def define_graph(self):\n",
    "        \"\"\"\n",
    "        This is a shallow model. so it does not really define a graph here \n",
    "        but it instantiates a model class with the classfier top.\n",
    "        \"\"\"     \n",
    "        self.num_classes = len(self.class2index)   # class2index is set in fit()\n",
    "        print(f\"define_graph: num_classes: {self.num_classes}\")\n",
    "        return TorchShallowClassifierModel(\n",
    "            self.input_embedding_size, \n",
    "            self.num_classes, \n",
    "            self.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, X=None):\n",
    "        \"\"\"\n",
    "        X: token embeddings\n",
    "        \n",
    "        attention_mask:\n",
    "            This argument is only needed if you want to compute the training loss\n",
    "            \n",
    "        labels: \n",
    "            2D tensor (batch_size, max_sequence_length), \n",
    "            each element is a class index in the range [0, C-1], no one-hot encodding required.\n",
    "            In mini-batch processing mode, the labels vectors must be padded \n",
    "            up to max_sequence_length.\n",
    "            This argument is only needed if you want to compute the training loss\n",
    "        \"\"\"\n",
    "        \n",
    "        # call the forward method on the classification model\n",
    "        logits = self.model.forward(X=X)                \n",
    "        outputs = (logits,) \n",
    "\n",
    "\n",
    "        return outputs  # (loss), logits\n",
    "\n",
    "    def compute_loss(self, logits, attention_mask, labels):\n",
    "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "        active_logits = logits.view(-1, self.num_classes)\n",
    "\n",
    "        if attention_mask is not None:              \n",
    "            # computes a boolean mask (flat boolean array), one element for each token of each row\n",
    "            active_loss_mask = attention_mask.view(-1) == 1\n",
    "            active_labels = torch.where(\n",
    "                active_loss_mask, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "            )\n",
    "        else:\n",
    "            active_labels = labels.view(-1)\n",
    "\n",
    "        # computes the loss between labels and logits \n",
    "        loss = loss_fct(active_logits, active_labels)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\"Standard `fit` method.\n",
    "        \n",
    "        fit() expects embeddings in X and strings in y.\n",
    "        The class itself is in charge of encoding the labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : [embeddings, attention_mask]\n",
    "        y : array-like, a list of lists of string [['O', 'B-fromcity']]\n",
    "        kwargs : dict\n",
    "            For passing other parameters. If 'X_dev' is included,\n",
    "            then performance is monitored every 10 epochs; use\n",
    "            `dev_iter` to control this number.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "\n",
    "        \"\"\"               \n",
    "        \n",
    "        ################################################################\n",
    "        # Model definition\n",
    "        ################################################################\n",
    "        \n",
    "        # Graph:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            \n",
    "            self.compute_class2index(y)  # expects strings, must run before tensorizing y\n",
    "            self.model = self.define_graph()\n",
    "        \n",
    "        # Prime the model for training\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        \n",
    "        # Default is torch.optim.Adam\n",
    "        optimizer = self.optimizer(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.l2_strength)    \n",
    "          \n",
    "        \n",
    "        ################################################################\n",
    "        # Data \n",
    "        ################################################################\n",
    "        # separate the feature vectors (embeddings) from the attention_mask\n",
    "        X, attention_mask = X\n",
    "        print(f\"X: {type(X)},     attention_mask: {type(attention_mask)},    y: [{len(y)}, {len(y[0])}]\")\n",
    "         \n",
    "        # Compute Incremental performance:\n",
    "        X_dev = kwargs.get('X_dev')\n",
    "        if X_dev is not None:\n",
    "            # X_dev contains 2 parameters, X_dev, attention_mask_dev\n",
    "            # the dev mask stays as numpy as is not used for anything\n",
    "            # because the class does not compute dev loss, it just stores \n",
    "            # the dev predictions. This mask is required by predict but\n",
    "            # it is just returned in the predict results (never used)            \n",
    "            dev_iter = kwargs.get('dev_iter', 10)\n",
    "        \n",
    "        # encode labels (label vectorization). must run before tensorizing y\n",
    "        y = self.encode_labels(y)\n",
    "        y = self.pad_to_max_length(y, X.shape[:2]) # y must have the shape of first 2 dims of X\n",
    "\n",
    "        # cast data into PyTorch tensors\n",
    "        X = torch.FloatTensor(X)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.bool)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        # Wrap data into a dataset and use a Dataloader for batching\n",
    "        dataset = torch.utils.data.TensorDataset(X, attention_mask, y)\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, \n",
    "            shuffle=True, pin_memory=True) \n",
    "        \n",
    "\n",
    "        ################################################################\n",
    "        # Training process (Gradient Descent)\n",
    "        ################################################################\n",
    "        for iteration in range(1, self.config[\"max_iter\"]+1):\n",
    "            epoch_error = 0.0\n",
    "            for i, (X_batch, m_batch, y_batch) in enumerate(dataloader):\n",
    "                \n",
    "                # load the batch input tensors into GPU memory\n",
    "                X_batch = X_batch.to(self.device, non_blocking=True)\n",
    "                m_batch = m_batch.to(self.device, non_blocking=True)\n",
    "                \n",
    "                # call forward (mask is not used unless you want to compute the training loss)\n",
    "                logits = self.model.forward(\n",
    "                    X=[X_batch, m_batch])\n",
    "                \n",
    "                # load the batch label tensors into GPU memory\n",
    "                y_batch = y_batch.to(self.device, non_blocking=True)\n",
    "                \n",
    "                # compute the loss, the gradients and update the weights\n",
    "                err = self.compute_loss(logits, m_batch, y_batch)\n",
    "                epoch_error += err.item()\n",
    "                optimizer.zero_grad()\n",
    "                err.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Incremental predictions where possible:\n",
    "            if X_dev is not None and iteration > 0 and iteration % dev_iter == 0:\n",
    "                self.dev_predictions[iteration] = self.predict(X_dev)\n",
    "                self.model.train()\n",
    "            self.errors.append(epoch_error)\n",
    "            progress_bar(\n",
    "                \"Finished epoch {} of {}; error is {}\".format(\n",
    "                    iteration, self.config[\"max_iter\"], epoch_error))\n",
    "        return self\n",
    "\n",
    "\n",
    "    \n",
    "    def predict_flat(self, X):\n",
    "        \"\"\"Predicted classes for the examples in `X`. In flat format for metric functions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "        attention_mask: input mask\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probs: torch.tensor(batch_size * max_sequence_length, num_classes)\n",
    "        pred_class: numpy.array(batch_size * max_sequence_length)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        probs, preds, attention_mask = self.predict(X)\n",
    "        \n",
    "        # Flatten and apply mask\n",
    "        preds_flat = preds.flatten()\n",
    "        preds_flat = preds_flat[attention_mask.flatten() == 1]        \n",
    "        \n",
    "        probs_flat = probs.flatten()\n",
    "        probs_flat = probs_flat[attention_mask.flatten() == 1] \n",
    "        \n",
    "        return probs_flat, preds_flat\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicted classes for the examples in `X`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "        attention_mask: np.array \n",
    "            input mask\n",
    "            # the dev mask stays as numpy as is not used for anything\n",
    "            # because the class does not compute dev loss, it just stores \n",
    "            # the dev predictions. This mask is required by predict but\n",
    "            # it is just returned in the predict results (never used)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probs: torch.tensor(batch_size, max_sequence_length, num_classes)\n",
    "        pred_class: numpy.array(batch_size, max_sequence_length)\n",
    "        attention_mask\n",
    "\n",
    "        \"\"\"\n",
    "        # get the attention_mask to return it\n",
    "        _, attention_mask = X\n",
    "\n",
    "        # compute probabilities and predicted class\n",
    "        probs = self.predict_proba(X)\n",
    "\n",
    "        # compute predicted class, maximizing across the last dimension (classes)\n",
    "        _, pred_class_idx = torch.max(probs, dim=-1)\n",
    "        pred_class_idx = pred_class_idx.detach().cpu().numpy()\n",
    "        \n",
    "        # decode the class indices tocla class names (IOB_tag)\n",
    "        preds = [[self.index2class[i] for i in row_class_idx] \n",
    "                              for row_class_idx in pred_class_idx]\n",
    "        preds = np.array(preds)\n",
    "\n",
    "        return probs.detach().cpu().numpy(), preds, attention_mask\n",
    "        \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predicted probabilities for the examples in `X`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor(batch_size, max_sequence_length, num_classes)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Graph:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            # self.class2index must be defined in this case\n",
    "            self.model = self.define_graph()\n",
    "        \n",
    "        # prime the model for prediction-only mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            self.model.to(self.device)\n",
    "       \n",
    "            # cast input data into PyTorch tensors\n",
    "            x, attention_mask = X\n",
    "            x, attention_mask = torch.tensor(x), torch.tensor(attention_mask, dtype=torch.bool)\n",
    "\n",
    "            # load the input tensors into GPU memory\n",
    "            x = x.to(self.device)\n",
    "            attention_mask = attention_mask.to(self.device)\n",
    "            \n",
    "            # call forward \n",
    "            output = self.forward(\n",
    "                X=[x, attention_mask])\n",
    "            \n",
    "            logits = output[0]\n",
    "            \n",
    "            # compute probabilities and predicted class\n",
    "            probs = nn.Softmax(dim=-1)(logits) # normalize scores along the latest dimension\n",
    "            \n",
    "            return probs  # tensor (no_grad)\n",
    "        \n",
    "    def compute_class2index(self, y):\n",
    "        \"\"\"\n",
    "        y: 2-D list of (lists of) strings (iob_tags, not indices)\n",
    "        \n",
    "        expects strings, must run before tensorizing y,\n",
    "        must run before defining the graph because it computes\n",
    "        the output network output (num_classes)\n",
    "        \n",
    "        Note:\n",
    "        if the input type is incorrect and it results in a number of classes is incorrect (very high) \n",
    "        it will likely cause a CUDA-OUT-OF-MEMORY error\n",
    "        \"\"\"\n",
    "\n",
    "        # flat list of iob labels\n",
    "        iob_labels = []\n",
    "        for y_row in y:\n",
    "            iob_labels.extend(y_row)\n",
    "            \n",
    "        # create mapping\n",
    "        classes = sorted(set(iob_labels))\n",
    "        self.class2index = dict(zip(classes, range(len(classes))))\n",
    "        self.index2class = {i:c for c, i in self.class2index.items()}        \n",
    "        \n",
    "    def encode_labels(self, y):\n",
    "        \"\"\"\n",
    "        y: 2-D list of (lists of) strings (iob_tags, not indices)\n",
    "        \n",
    "        expects strings, must run before tensorizing y,\n",
    "        \"\"\"\n",
    "        tag_id_matrix = []\n",
    "        for iob_tags in y:\n",
    "            tag_ids = [self.class2index[iob_tag] for iob_tag in iob_tags]\n",
    "            tag_id_matrix.append(tag_ids)  \n",
    "\n",
    "        return  tag_id_matrix\n",
    "    \n",
    "    def pad_to_max_length(self, jagged_matrix, output_shape):\n",
    "        padded_matrix = np.zeros(shape=output_shape)\n",
    "        for i, row in enumerate(jagged_matrix):\n",
    "            padded_matrix[i, :len(row)] = row \n",
    "        return padded_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Training and Prediction of the Shallow Slot Filling model (the Shallow model uses Bert embeddings without fine-tunning)\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling: Alignment, Encoding, Normalize the length of the sequences, Class Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(class_ids):\n",
    "    \"\"\"\n",
    "        class_ids: 1D tensor, contains one class_id for each example\n",
    "    \"\"\"\n",
    "    # encode the class_ids as onehot\n",
    "    class_matrix = np.zeros(shape=(len(class_ids), max(class_ids)))\n",
    "    class_matrix[class_ids] = 1\n",
    "    \n",
    "    # set the positive weights as the fraction of negative labels (0) for each class (each column)\n",
    "    w_p = np.sum(class_matrix == 0, axis=0) / class_matrix.shape[0]\n",
    "\n",
    "    # set the negative weights as the fraction of positive labels (1) for each class (each column)\n",
    "    w_n = np.sum(class_matrix == 1, axis=0) / class_matrix.shape[0]\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label and Sub-token Alignment (call WordPiece for each token, output word_to_tok_map and aligned_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_start_label_aligner(sentence, word_labels=None):\n",
    "    \"\"\"\n",
    "    Aligns the IOB labels to the word-starting tokens in the list of\n",
    "    sub-word tokens returned by the WordPiece tokenizer.\n",
    "    Returns:\n",
    "    - an array of indices, each pointing to the first sub-token of every word\n",
    "    - a padded list of labels, which has one element for each sub-token (the first\n",
    "      sub-token of every word gets the label, the rest get the padding label 'X')\n",
    "    \"\"\"\n",
    "    # Token map will be an int -> int mapping between the `word` index in the sentence and\n",
    "    # the WordPiece `tokens` index.\n",
    "    word_start_indices = []\n",
    "    tokens = [\"[CLS]\"]\n",
    "    if word_labels is not None:\n",
    "        token_labels = [\"O\"]\n",
    "    else:\n",
    "        token_labels = None\n",
    "    if len(sentence.split()) != len(word_labels):\n",
    "        print(f\"sentence: {len(sentence.split())}, word_labels: {len(word_labels)}\")\n",
    "        print(f\"sentence: {sentence.split()}, word_labels: {word_labels}\")\n",
    "    for word_idx, word in enumerate(sentence.split(' ')):\n",
    "        word_start_indices.append(len(tokens))\n",
    "        word_tokens = hf_tokenizer.tokenize(word)  # tokenize ONE word \n",
    "        tokens.extend(word_tokens)\n",
    "        if word_labels is not None:\n",
    "            token_labels.append(word_labels[word_idx])\n",
    "            if len(word_tokens) > 1:\n",
    "                token_labels.extend([\"X\"]*(len(word_tokens)-1))\n",
    "\n",
    "    tokens.append(\"[SEP]\")\n",
    "    token_labels.append( \"O\")\n",
    "    return token_labels, word_start_indices, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_label_aligner(sentences, labels):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    label_matrix, word_start_matrix, token_matrix = [], [], []\n",
    "    \n",
    "    for sentence, word_labels in zip(sentences, labels):\n",
    "        token_labels, word_start_indices, tokens =\\\n",
    "            word_start_label_aligner(sentence, word_labels)\n",
    "        \n",
    "        label_matrix.append(token_labels)\n",
    "        word_start_matrix.append(word_start_indices)\n",
    "        token_matrix.append(tokens)\n",
    "\n",
    "    return label_matrix, word_start_matrix, token_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_predict_output(y_true, preds, attention_mask):\n",
    "    \"\"\"\n",
    "    y_true : list of list of strings (IOB_tags)\n",
    "        y_true is a list of  variable-length lists of strings, \n",
    "        is token-alignmed but not lenght-padded\n",
    "        \n",
    "    preds : np.array(batch_size, max_sentence_length)\n",
    "        2-D array with the class predicted by the model,\n",
    "        for each token of each sentence.\n",
    "        The attention mask must be applied to exclude the padding tokens.\n",
    "        \n",
    "    attention_mask: np.array(batch_size, max_sentence_length)\n",
    "        boolean tensor to filter the padding tokens\n",
    "    \n",
    "    In order to produce a classification report for sequence tagging, \n",
    "    first al the arrays need to be flattened.\n",
    "    \"\"\"\n",
    "\n",
    "    # flatten the sequence labels\n",
    "    y_flat = []\n",
    "    for iob_tags in y_true:\n",
    "        y_flat.extend(iob_tags)\n",
    "\n",
    "    # apply mask to remove padding token positions and flatten the matrix\n",
    "    preds_flat = preds.flatten()\n",
    "    preds_flat = preds_flat[attention_mask.flatten() == 1]\n",
    "    \n",
    "    return y_flat, preds_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_classification_report(y, predict_output, digits=3):\n",
    "    \"\"\"\n",
    "    Adapts the interface between the experiment and the sequence-tagging report function\n",
    "    y : non-padded token_label_matrix\n",
    "        list of list of strings (IOB_tags)\n",
    "        y is a list of  variable-length lists of strings, \n",
    "        is token-alignmed but not lenght-padded\n",
    "        \n",
    "    preds : np.array(batch_size, max_sentence_length)\n",
    "        2-D array with the class predicted by the model,\n",
    "        for each token of each sentence.\n",
    "        The attention mask must be applied to exclude the padding tokens.\n",
    "        \n",
    "    attention_mask:\n",
    "        boolean tensor to filter the padding tokens\n",
    "    \"\"\"\n",
    "    probs, preds, attention_mask = predict_output[:3]\n",
    "        \n",
    "    y_flat, preds_flat =\\\n",
    "        flatten_predict_output(y, preds, attention_mask)\n",
    "\n",
    "    print(classification_report(y_flat, preds_flat, digits=digits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_macro_f1(y, predict_output, digits=3):\n",
    "    \"\"\"\n",
    "    Adapts the interface between the experiment and the sequence-tagging scoring function\n",
    "    y : non-padded token_label_matrix\n",
    "        list of list of strings (IOB_tags)\n",
    "        y is a list of  variable-length lists of strings, \n",
    "        is token-alignmed but not lenght-padded\n",
    "        \n",
    "        \n",
    "    preds : np.array(batch_size, max_sentence_length)\n",
    "        2-D array with the class predicted by the model,\n",
    "        for each token of each sentence.\n",
    "        The attention mask must be applied to exclude the padding tokens.\n",
    "        \n",
    "    attention_mask:\n",
    "        boolean tensor to filter the padding tokens\n",
    "    \"\"\"\n",
    "    probs, preds, attention_mask = predict_output[:3]\n",
    "    \n",
    "    y_flat, pred_flat =\\\n",
    "        flatten_predict_output(y, preds, attention_mask)\n",
    "    \n",
    "    return utils.safe_macro_f1(y_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A feed-forward experiment with the ATIS module\n",
    "* * *\n",
    "\n",
    "It is straightforward to conduct experiments like the above using `atis.experiment`, which will enable you to do a wider range of experiments without writing or copy-pasting a lot of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_fit(X,  y):\n",
    "    \"\"\"\n",
    "        X : contains the embeddings and the attention_mask\n",
    "            X[0]: embeddings torchTensor([batch_size, max_sentence_length, 768])\n",
    "            fit() expects a tensor of embeddings\n",
    "        \n",
    "            X[1]: attention_mask: torchTensor([batch_size, max_sentence_length])\n",
    "            expects the tensor input_mask_train\n",
    "            it is used to mask the padding tokens in X\n",
    "            each value is 0 or 1.\n",
    "\n",
    "        y: torchTensor([batch_size, max_sentence_length])\n",
    "            expects the tensor padded_token_label_matrix_train\n",
    "            each value is a class string (IOB_tag)\n",
    "    \"\"\"        \n",
    "\n",
    "    # configures the sequence tagging layer\n",
    "    sequence_tagging_config = {\n",
    "        \"input_embedding_size\": 768,\n",
    "        \"hidden_dropout_prob\": 0.4,\n",
    "        \"class_weights\": None,\n",
    "        \"batch_size\": 64,\n",
    "        \"lr\": 1e-3,\n",
    "        \"l2_strength\": 0,\n",
    "        \"max_iter\": 50,   # keep small during debug\n",
    "        \"device\": \"cuda\"\n",
    "    }\n",
    "    \n",
    "    # instantiates the network\n",
    "    shallow_sf = TorchShallowSequenceTagger(sequence_tagging_config)\n",
    "\n",
    "    # Fit\n",
    "    return shallow_sf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_phi(sentences):\n",
    "    \"\"\"\n",
    "    transformer-based batch tokenzer, encoder, vectorizer, and sequence padding\n",
    "    \"\"\"\n",
    "\n",
    "    final_hidden_states, attention_mask, input_token_ids =\\\n",
    "        batch_encoder_vectorizer(sentences)\n",
    "    \n",
    "    return final_hidden_states, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define_graph: num_classes: 121\n",
      "X: <class 'numpy.ndarray'>,     attention_mask: <class 'numpy.ndarray'>,    y: [4478, 16]\n",
      "X: (4478, 63, 768),    attention_mask: (4478, 63),    y: (4478, 63)\n",
      "X: torch.Size([4478, 63, 768]),    attention_mask: torch.Size([4478, 63]),    y: torch.Size([4478, 63])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 4.8276680856943135"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(y_true): 500\n",
      "preds.shape: (500, 47)\n",
      "attention_mask.shape: (500, 47)\n",
      "y_flat (flattened): 9348\n",
      "preds_flat.shape (flattened): (23500,)\n",
      "preds_flat.shape (masked): (9348,)\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "             B-aircraft_code      0.000     0.000     0.000         1\n",
      "              B-airline_code      1.000     0.778     0.875         9\n",
      "              B-airline_name      0.969     1.000     0.984        62\n",
      "              B-airport_code      1.000     0.750     0.857         4\n",
      "              B-airport_name      1.000     0.750     0.857         4\n",
      " B-arrive_date.date_relative      0.000     0.000     0.000         2\n",
      "      B-arrive_date.day_name      0.000     0.000     0.000        10\n",
      "    B-arrive_date.day_number      0.500     0.250     0.333         4\n",
      "    B-arrive_date.month_name      1.000     0.250     0.400         4\n",
      "B-arrive_date.today_relative      0.000     0.000     0.000         1\n",
      "      B-arrive_time.end_time      1.000     0.667     0.800         3\n",
      "    B-arrive_time.period_mod      0.000     0.000     0.000         1\n",
      " B-arrive_time.period_of_day      1.000     0.231     0.375        13\n",
      "    B-arrive_time.start_time      1.000     0.667     0.800         3\n",
      "          B-arrive_time.time      0.722     0.765     0.743        17\n",
      " B-arrive_time.time_relative      0.812     0.929     0.867        14\n",
      "                 B-city_name      0.952     0.870     0.909        23\n",
      "                B-class_type      1.000     1.000     1.000        23\n",
      "                   B-connect      1.000     1.000     1.000         4\n",
      "             B-cost_relative      0.971     1.000     0.985        33\n",
      "                  B-day_name      0.000     0.000     0.000         2\n",
      "                 B-days_code      1.000     1.000     1.000         1\n",
      " B-depart_date.date_relative      0.429     0.500     0.462         6\n",
      "      B-depart_date.day_name      0.897     1.000     0.945       104\n",
      "    B-depart_date.day_number      0.886     0.975     0.929        40\n",
      "    B-depart_date.month_name      0.889     1.000     0.941        40\n",
      "B-depart_date.today_relative      1.000     1.000     1.000         9\n",
      "          B-depart_date.year      1.000     1.000     1.000         4\n",
      "      B-depart_time.end_time      0.500     1.000     0.667         1\n",
      "    B-depart_time.period_mod      0.800     0.889     0.842         9\n",
      " B-depart_time.period_of_day      0.829     0.944     0.883        72\n",
      "    B-depart_time.start_time      0.500     1.000     0.667         1\n",
      "          B-depart_time.time      0.860     0.881     0.871        42\n",
      " B-depart_time.time_relative      0.923     0.900     0.911        40\n",
      "                   B-economy      1.000     1.000     1.000         2\n",
      "               B-fare_amount      1.000     0.714     0.833         7\n",
      "           B-fare_basis_code      0.929     1.000     0.963        13\n",
      "               B-flight_days      1.000     0.500     0.667         2\n",
      "                B-flight_mod      0.925     0.925     0.925        40\n",
      "             B-flight_number      1.000     1.000     1.000         6\n",
      "               B-flight_stop      1.000     0.920     0.958        25\n",
      "               B-flight_time      1.000     0.833     0.909        12\n",
      "      B-fromloc.airport_code      1.000     1.000     1.000         1\n",
      "      B-fromloc.airport_name      0.778     0.438     0.560        16\n",
      "         B-fromloc.city_name      0.952     0.956     0.954       434\n",
      "        B-fromloc.state_code      1.000     1.000     1.000         5\n",
      "        B-fromloc.state_name      0.800     1.000     0.889         4\n",
      "                      B-meal      1.000     1.000     1.000         3\n",
      "          B-meal_description      1.000     1.000     1.000         8\n",
      "                       B-mod      0.000     0.000     0.000         1\n",
      "                        B-or      1.000     0.571     0.727         7\n",
      "             B-period_of_day      0.000     0.000     0.000         2\n",
      "          B-restriction_code      1.000     1.000     1.000         2\n",
      " B-return_date.date_relative      0.750     1.000     0.857         3\n",
      "    B-return_date.day_number      0.000     0.000     0.000         2\n",
      "    B-return_date.month_name      0.000     0.000     0.000         2\n",
      " B-return_time.period_of_day      0.000     0.000     0.000         1\n",
      "                B-round_trip      1.000     1.000     1.000        25\n",
      "                B-state_code      0.000     0.000     0.000         1\n",
      "                B-state_name      0.000     0.000     0.000         1\n",
      "         B-stoploc.city_name      0.857     0.857     0.857        21\n",
      "        B-toloc.airport_code      1.000     0.500     0.667         4\n",
      "        B-toloc.airport_name      0.500     0.500     0.500         4\n",
      "           B-toloc.city_name      0.940     0.967     0.953       424\n",
      "          B-toloc.state_code      0.909     1.000     0.952        10\n",
      "          B-toloc.state_name      0.833     0.714     0.769         7\n",
      "            B-transport_type      1.000     1.000     1.000         8\n",
      "              I-airline_name      1.000     0.976     0.988        42\n",
      "              I-airport_name      0.667     1.000     0.800         4\n",
      "      I-arrive_time.end_time      1.000     0.667     0.800         3\n",
      "          I-arrive_time.time      0.750     0.750     0.750        12\n",
      "                 I-city_name      0.750     0.750     0.750         4\n",
      "                I-class_type      1.000     1.000     1.000        19\n",
      "             I-cost_relative      1.000     1.000     1.000         6\n",
      "    I-depart_date.day_number      0.929     1.000     0.963        13\n",
      "      I-depart_time.end_time      1.000     1.000     1.000         1\n",
      " I-depart_time.period_of_day      0.000     0.000     0.000         1\n",
      "    I-depart_time.start_time      1.000     1.000     1.000         1\n",
      "          I-depart_time.time      0.917     0.943     0.930        35\n",
      "               I-fare_amount      1.000     1.000     1.000         6\n",
      "           I-fare_basis_code      0.000     0.000     0.000         1\n",
      "                I-flight_mod      0.000     0.000     0.000         2\n",
      "               I-flight_stop      1.000     1.000     1.000         2\n",
      "               I-flight_time      1.000     0.833     0.909         6\n",
      "      I-fromloc.airport_name      1.000     0.636     0.778        22\n",
      "         I-fromloc.city_name      0.929     0.881     0.904        59\n",
      "        I-fromloc.state_name      0.500     1.000     0.667         1\n",
      "          I-restriction_code      1.000     1.000     1.000         1\n",
      " I-return_date.date_relative      0.000     0.000     0.000         0\n",
      "    I-return_date.day_number      0.000     0.000     0.000         1\n",
      "                I-round_trip      1.000     1.000     1.000        25\n",
      "         I-stoploc.city_name      1.000     0.667     0.800         6\n",
      "        I-toloc.airport_name      0.500     0.667     0.571         3\n",
      "           I-toloc.city_name      0.922     0.955     0.939       112\n",
      "          I-toloc.state_name      0.000     0.000     0.000         1\n",
      "            I-transport_type      1.000     1.000     1.000         2\n",
      "                           O      0.992     0.997     0.994      4603\n",
      "                           X      0.997     0.998     0.997      2645\n",
      "\n",
      "                    accuracy                          0.979      9348\n",
      "                   macro avg      0.729     0.695     0.698      9348\n",
      "                weighted avg      0.976     0.979     0.976      9348\n",
      "\n",
      "None\n",
      "len(y_true): 500\n",
      "preds.shape: (500, 47)\n",
      "attention_mask.shape: (500, 47)\n",
      "y_flat (flattened): 9348\n",
      "preds_flat.shape (flattened): (23500,)\n",
      "preds_flat.shape (masked): (9348,)\n",
      "CPU times: user 16min 52s, sys: 1min 35s, total: 18min 28s\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "exp_results = atis.experiment(\n",
    "    ATIS_HOME,\n",
    "    phi=None,\n",
    "    batch_phi=sequence_tagging_phi,\n",
    "    label_alignment_func=sequence_tagging_label_aligner,\n",
    "    train_func=sequence_tagging_fit,\n",
    "    train_reader=atis.train_reader, \n",
    "    assess_reader=atis.dev_reader, \n",
    "    class_func=atis.slot_filling_func,   # label selector\n",
    "    metrics_report_func=sequence_tagging_classification_report,\n",
    "    score_func=sequence_tagging_macro_f1,\n",
    "    vectorize=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSequenceTaggingModel(nn.Module):\n",
    "    def __init__(self,\n",
    "            output_dim,\n",
    "            hidden_dropout_prob,\n",
    "            weights_name='bert-base-cased'):\n",
    "        super(BertSequenceTaggingModel, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.weights_name = weights_name\n",
    "        \n",
    "        # Graph\n",
    "        self.bert = BertModel.from_pretrained(self.weights_name)\n",
    "        self.embed_dim = self.bert.embeddings.word_embeddings.embedding_dim \n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)  \n",
    "        self.classifier_layer = nn.Linear(self.embed_dim, output_dim)\n",
    "        \n",
    "        # init weights\n",
    "        torch.nn.init.xavier_uniform(self.classifier_layer.weight) \n",
    "        if self.classifier_layer.bias is not None:\n",
    "            self.classifier_layer.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Here, `X` is a list of two np.array  \n",
    "        consisting of the token_ids (an index into the BERT embedding)\n",
    "        and the attention_mask (a 1 or 0 indicating whether the token \n",
    "        is masked). The `fit` method will \n",
    "        train all these parameters against a softmax objective.\n",
    "        \n",
    "        \"\"\"\n",
    "        # separates the indices from the mask\n",
    "        indices, mask = X\n",
    "        # Type conversion, since the base class insists on\n",
    "        # casting this as a FloatTensor, but we ned Long\n",
    "        # for `bert`.\n",
    "        indices = indices.long()\n",
    "        \n",
    "        # graph execution\n",
    "        final_hidden_states, cls_output =\\\n",
    "            self.bert(indices, attention_mask=mask)\n",
    "        \n",
    "        h = self.dropout(final_hidden_states)\n",
    "        \n",
    "        logits = self.classifier_layer(h)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSequenceTagging(TorchShallowSequenceTagger):\n",
    "    def __init__(self, weights_name, *args, **kwargs):\n",
    "        self.weights_name = weights_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.weights_name)\n",
    "        super(BertSequenceTagging, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def define_graph(self):\n",
    "        \"\"\"This method is used by `fit`. We override it here to use our\n",
    "        new BERT-based graph.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.num_classes = len(self.class2index)   # class2index is set in fit()\n",
    "        model = BertSequenceTaggingModel(\n",
    "            output_dim=self.num_classes,\n",
    "            hidden_dropout_prob=self.hidden_dropout_prob, \n",
    "            weights_name=self.weights_name)\n",
    "        model.train() # flag\n",
    "        return model\n",
    "    \n",
    "    def encode(self, X, max_length=None):\n",
    "        \"\"\"The `X` is a list of strings. We use the model's tokenizer\n",
    "        to get the indices and mask information.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list of [index, mask] pairs, where index is an int and mask\n",
    "        is 0 or 1.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ## IN FINE TUNNIG WE DEAL WITH TOKEN_IDS (indices)\n",
    "        \n",
    "        data = self.tokenizer.batch_encode_plus(\n",
    "            X, \n",
    "            max_length=max_length,\n",
    "            add_special_tokens=True, \n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True)\n",
    "        indices = np.array(data['input_ids'])\n",
    "        mask = np.array(data['attention_mask'])\n",
    "        \n",
    "        return [indices, mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a self-contained illustration, starting from the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train = list(atis.train_reader(ATIS_HOME, class_func=atis.slot_filling_func))\n",
    "hf_dev = list(atis.dev_reader(ATIS_HOME, class_func=atis.slot_filling_func))\n",
    "\n",
    "X_hf_sentence_train, y_train = zip(*hf_train)\n",
    "X_hf_sentence_dev, y_dev = zip(*hf_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has some standard fine-tuning parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configures the sequence tagging layer\n",
    "sequence_tagging_config = {\n",
    "    \"input_embedding_size\": None, # not needed for fine-tunning\n",
    "    \"hidden_dropout_prob\": 0.4,\n",
    "    \"class_weights\": None,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 0.0002,   # eta\n",
    "    \"l2_strength\": 0,\n",
    "    \"max_iter\": 4,   # keep small during debug\n",
    "    \"device\": \"cuda\"\n",
    "}\n",
    "\n",
    "\n",
    "hf_fine_tune_mod = BertSequenceTagging(\n",
    "    'bert-base-cased', \n",
    "    config=sequence_tagging_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can encode them; this step packs together the indices and mask information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_indices_mask_train = hf_fine_tune_mod.encode(X_hf_sentence_train)\n",
    "\n",
    "X_indices_mask_dev = hf_fine_tune_mod.encode(X_hf_sentence_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.]]]), tensor([[False, False],\n",
       "         [False, False]])]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [np.ones(shape=(2, 2, 5)), np.zeros(shape=(2, 2))]\n",
    "x, m = X\n",
    "X = [torch.FloatTensor(x),torch.tensor(m, dtype=torch.bool)]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this model is resource intensive. Be patient – it will be worth the wait! (This experiment takes about 10 minutes on a machine with an NVIDIA RTX 2080 Max-Q GPU.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, _, _ =  sequence_tagging_label_aligner(X_hf_sentence_train, y_train)\n",
    "\n",
    "y_dev, _, _ =  sequence_tagging_label_aligner(X_hf_sentence_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: <class 'numpy.ndarray'>,     attention_mask: <class 'numpy.ndarray'>,    y: [4478, 16]\n",
      "X: (4478, 63),    attention_mask: (4478, 63),    y: (4478, 63)\n",
      "X: torch.Size([4478, 63]),    attention_mask: torch.Size([4478, 63]),    y: torch.Size([4478, 63])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 4 of 4; error is 4.129124558996409"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 30s, sys: 52.6 s, total: 4min 22s\n",
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%time _ = hf_fine_tune_mod.fit(X_indices_mask_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some predictions on the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_fine_tune_preds = hf_fine_tune_mod.predict(X_indices_mask_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(y_true): 500\n",
      "preds.shape: (500, 47)\n",
      "attention_mask.shape: (500, 47)\n",
      "y_flat (flattened): 9348\n",
      "preds_flat.shape (flattened): (23500,)\n",
      "preds_flat.shape (masked): (9348,)\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "             B-aircraft_code      1.000     1.000     1.000         1\n",
      "              B-airline_code      0.900     1.000     0.947         9\n",
      "              B-airline_name      1.000     1.000     1.000        62\n",
      "              B-airport_code      1.000     0.750     0.857         4\n",
      "              B-airport_name      1.000     1.000     1.000         4\n",
      " B-arrive_date.date_relative      1.000     0.500     0.667         2\n",
      "      B-arrive_date.day_name      1.000     0.500     0.667        10\n",
      "    B-arrive_date.day_number      1.000     1.000     1.000         4\n",
      "    B-arrive_date.month_name      1.000     0.750     0.857         4\n",
      "B-arrive_date.today_relative      0.000     0.000     0.000         1\n",
      "      B-arrive_time.end_time      0.500     0.333     0.400         3\n",
      "    B-arrive_time.period_mod      0.000     0.000     0.000         1\n",
      " B-arrive_time.period_of_day      0.833     0.769     0.800        13\n",
      "    B-arrive_time.start_time      1.000     0.333     0.500         3\n",
      "          B-arrive_time.time      0.800     0.941     0.865        17\n",
      " B-arrive_time.time_relative      0.667     0.857     0.750        14\n",
      "                 B-city_name      0.840     0.913     0.875        23\n",
      "                B-class_type      1.000     1.000     1.000        23\n",
      "                   B-connect      1.000     0.750     0.857         4\n",
      "             B-cost_relative      1.000     1.000     1.000        33\n",
      "                  B-day_name      0.000     0.000     0.000         2\n",
      "                 B-days_code      0.000     0.000     0.000         1\n",
      " B-depart_date.date_relative      0.500     0.500     0.500         6\n",
      "      B-depart_date.day_name      0.936     0.990     0.963       104\n",
      "    B-depart_date.day_number      0.952     1.000     0.976        40\n",
      "    B-depart_date.month_name      0.951     0.975     0.963        40\n",
      "B-depart_date.today_relative      0.889     0.889     0.889         9\n",
      "          B-depart_date.year      1.000     1.000     1.000         4\n",
      "      B-depart_time.end_time      1.000     1.000     1.000         1\n",
      "    B-depart_time.period_mod      0.875     0.778     0.824         9\n",
      " B-depart_time.period_of_day      0.921     0.972     0.946        72\n",
      "    B-depart_time.start_time      1.000     1.000     1.000         1\n",
      "          B-depart_time.time      0.933     1.000     0.966        42\n",
      " B-depart_time.time_relative      0.870     1.000     0.930        40\n",
      "                   B-economy      1.000     1.000     1.000         2\n",
      "               B-fare_amount      1.000     0.857     0.923         7\n",
      "           B-fare_basis_code      1.000     0.923     0.960        13\n",
      "               B-flight_days      1.000     1.000     1.000         2\n",
      "                B-flight_mod      0.950     0.950     0.950        40\n",
      "             B-flight_number      1.000     1.000     1.000         6\n",
      "               B-flight_stop      1.000     0.920     0.958        25\n",
      "               B-flight_time      1.000     1.000     1.000        12\n",
      "      B-fromloc.airport_code      1.000     1.000     1.000         1\n",
      "      B-fromloc.airport_name      1.000     0.938     0.968        16\n",
      "         B-fromloc.city_name      0.998     0.972     0.985       434\n",
      "        B-fromloc.state_code      1.000     1.000     1.000         5\n",
      "        B-fromloc.state_name      1.000     1.000     1.000         4\n",
      "                      B-meal      1.000     1.000     1.000         3\n",
      "          B-meal_description      1.000     1.000     1.000         8\n",
      "                       B-mod      0.000     0.000     0.000         1\n",
      "                        B-or      0.778     1.000     0.875         7\n",
      "             B-period_of_day      0.000     0.000     0.000         2\n",
      "          B-restriction_code      1.000     1.000     1.000         2\n",
      " B-return_date.date_relative      1.000     1.000     1.000         3\n",
      "    B-return_date.day_number      0.000     0.000     0.000         2\n",
      "    B-return_date.month_name      0.000     0.000     0.000         2\n",
      " B-return_time.period_of_day      0.000     0.000     0.000         1\n",
      "                B-round_trip      0.926     1.000     0.962        25\n",
      "                B-state_code      1.000     1.000     1.000         1\n",
      "                B-state_name      0.000     0.000     0.000         1\n",
      "         B-stoploc.city_name      0.955     1.000     0.977        21\n",
      "        B-toloc.airport_code      1.000     1.000     1.000         4\n",
      "        B-toloc.airport_name      0.667     0.500     0.571         4\n",
      "           B-toloc.city_name      0.964     0.998     0.980       424\n",
      "          B-toloc.state_code      1.000     1.000     1.000        10\n",
      "          B-toloc.state_name      1.000     0.857     0.923         7\n",
      "            B-transport_type      0.889     1.000     0.941         8\n",
      "              I-airline_name      1.000     1.000     1.000        42\n",
      "              I-airport_name      1.000     1.000     1.000         4\n",
      "      I-arrive_time.end_time      1.000     1.000     1.000         3\n",
      "          I-arrive_time.time      1.000     1.000     1.000        12\n",
      "                 I-city_name      0.750     0.750     0.750         4\n",
      "                I-class_type      1.000     1.000     1.000        19\n",
      "             I-cost_relative      1.000     1.000     1.000         6\n",
      "    I-depart_date.day_number      0.929     1.000     0.963        13\n",
      "I-depart_date.today_relative      0.000     0.000     0.000         0\n",
      "      I-depart_time.end_time      1.000     1.000     1.000         1\n",
      " I-depart_time.period_of_day      1.000     1.000     1.000         1\n",
      "    I-depart_time.start_time      1.000     1.000     1.000         1\n",
      "          I-depart_time.time      0.971     0.971     0.971        35\n",
      "               I-fare_amount      1.000     1.000     1.000         6\n",
      "           I-fare_basis_code      0.000     0.000     0.000         1\n",
      "                I-flight_mod      0.500     0.500     0.500         2\n",
      "               I-flight_stop      1.000     1.000     1.000         2\n",
      "               I-flight_time      0.750     1.000     0.857         6\n",
      "      I-fromloc.airport_name      1.000     0.864     0.927        22\n",
      "         I-fromloc.city_name      0.951     0.983     0.967        59\n",
      "        I-fromloc.state_name      1.000     1.000     1.000         1\n",
      "          I-restriction_code      0.500     1.000     0.667         1\n",
      "    I-return_date.day_number      0.000     0.000     0.000         1\n",
      "                I-round_trip      1.000     1.000     1.000        25\n",
      "         I-stoploc.city_name      1.000     0.833     0.909         6\n",
      "        I-toloc.airport_name      0.400     0.667     0.500         3\n",
      "           I-toloc.city_name      0.973     0.982     0.978       112\n",
      "          I-toloc.state_name      1.000     1.000     1.000         1\n",
      "            I-transport_type      1.000     1.000     1.000         2\n",
      "                           O      0.998     0.995     0.997      4603\n",
      "                           X      1.000     0.999     0.999      2645\n",
      "\n",
      "                    accuracy                          0.989      9348\n",
      "                   macro avg      0.808     0.796     0.795      9348\n",
      "                weighted avg      0.988     0.989     0.988      9348\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sequence_tagging_classification_report(y_dev, hf_fine_tune_preds, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
