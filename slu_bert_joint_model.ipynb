{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU Joint Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Adrian Sarno, Jennifer Arnold\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2020\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all the random seeds for reproducibility. Only the\n",
    "# system and torch seeds are relevant for this notebook.\n",
    "import utils\n",
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.level = logging.ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace import\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import atis\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "from torch_rnn_classifier import TorchRNNClassifier, TorchRNNClassifierModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n",
      "1.4.0 True\n",
      "GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "# CUDA test\n",
    "import sys; print(sys.version)\n",
    "import torch; print(torch.__version__, torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hf_weights_name = 'bert-base-cased'\n",
    "# hf_weights_name = 'bert-base-uncased' - in this case the tokenizer does not split into subwords so often\n",
    "\n",
    "hf_tokenizer = BertTokenizer.from_pretrained(hf_weights_name)\n",
    "hf_model = BertModel.from_pretrained(hf_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATIS_HOME = os.path.join(\"data\", \"atis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching (normalizing sentence lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encoder_vectorizer(input_sentences, max_length=None):\n",
    "    \"\"\"\n",
    "    This function accomplishes two tasks:\n",
    "    1.  tokenization and sentence-length normalization\n",
    "    2.  featurization, it calls the bert model to convert tokens to embeddings \n",
    "    \"\"\"\n",
    "    \n",
    "    # tokenization, encoding and sentence-length normalization\n",
    "    tokenizer_output = hf_tokenizer.batch_encode_plus(\n",
    "        input_sentences, \n",
    "        max_length=max_length,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True)\n",
    "    \n",
    "    input_token_ids = torch.tensor(tokenizer_output['input_ids'])\n",
    "    input_mask = torch.tensor(tokenizer_output['attention_mask'])\n",
    "\n",
    "    # featurization\n",
    "    with torch.no_grad():\n",
    "        final_hidden_states, cls_output = \\\n",
    "        hf_model(input_token_ids, attention_mask=input_mask)\n",
    "    \n",
    "    # cls_output not used\n",
    "    # convert to numpy to match the type of all other results (all numpy)\n",
    "    final_hidden_states = final_hidden_states.detach().cpu().numpy()\n",
    "    \n",
    "    return final_hidden_states, np.array(tokenizer_output['attention_mask']), np.array(tokenizer_output['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchJointSluModel(nn.Module):\n",
    "    def __init__(self,\n",
    "            embed_dim,\n",
    "            output_iob_dim,\n",
    "            output_intent_dim,\n",
    "            dropout_prob):\n",
    "        super(TorchJointSluModel, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.output_iob_dim = output_iob_dim\n",
    "        self.output_intent_dim = output_intent_dim\n",
    "        \n",
    "        # Graph\n",
    "        self.dropout = nn.Dropout(dropout_prob) \n",
    "        \n",
    "        self.iob_classifier_layer = nn.Linear(embed_dim, output_iob_dim)\n",
    "        self.init_weights(self.iob_classifier_layer) \n",
    "\n",
    "        self.intent_classifier_layer = nn.Linear(embed_dim, output_intent_dim)\n",
    "        self.init_weights(self.intent_classifier_layer) \n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        torch.nn.init.xavier_uniform(layer.weight) \n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.zero_()        \n",
    "            \n",
    "    def forward(self, X):\n",
    "    \n",
    "        # separate the feature vectors (embeddings) from the attention_mask\n",
    "        X, attention_mask = X\n",
    "\n",
    "        X = self.dropout(X)\n",
    "        \n",
    "        logits_iob = self.iob_classifier_layer(X)\n",
    "        \n",
    "        # create single embedding per sentence\n",
    "        sentence_X = X.mean(axis=1)  # for better performance\n",
    "        \n",
    "        logits_intent = self.intent_classifier_layer(sentence_X)\n",
    "\n",
    "        return logits_iob, logits_intent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_model_base import TorchModelBase\n",
    "from utils import progress_bar\n",
    "\n",
    "class TorchJointSlu(TorchModelBase):\n",
    "    \"\"\"\n",
    "     Featurization:\n",
    "        Takes the embeddings already pre-computed.\n",
    "    \n",
    "    Classification:\n",
    "        The simplest token classifier uses just a linear layer.\n",
    "        The Pytorch linear layer can take as input a tensor of any number of dimensions\n",
    "        and only the last dimension needs to be specified as input dimension.\n",
    "   \n",
    "        https://pytorch.org/docs/master/nn.html#linear   \n",
    "    \"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super(TorchJointSlu, self).__init__(**kwargs)\n",
    "        \n",
    "        self.config = config\n",
    "        self.input_embedding_size = config[\"input_embedding_size\"]\n",
    "        self.hidden_dropout_prob = config[\"hidden_dropout_prob\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.l2_strength = config[\"l2_strength\"]\n",
    "        self.max_iter = config[\"max_iter\"]\n",
    "        self.device = config[\"device\"]\n",
    "        self.class_weights = config.get(\"class_weights\", None)\n",
    "        if self.class_weights is not None:\n",
    "            class_weights = torch.FloatTensor(self.class_weights)\n",
    "        \n",
    "    def define_graph(self):\n",
    "        \"\"\"\n",
    "        This is a shallow model. so it does not really define a graph here \n",
    "        but it instantiates a model class with the classfier top.\n",
    "        \"\"\"     \n",
    "        self.num_iob_classes = len(self.iob_class2index)   # class2index is set in fit()\n",
    "        self.num_intent_classes = len(self.intent_class2index)   # class2index is set in fit()\n",
    "        print(f\"define_graph: num_iob_classes: {self.num_iob_classes}\")\n",
    "        print(f\"define_graph: num_intent_classes: {self.num_intent_classes}\")\n",
    "        return TorchJointSluModel(\n",
    "            self.input_embedding_size, \n",
    "            self.num_iob_classes, \n",
    "            self.num_intent_classes, \n",
    "            self.hidden_dropout_prob)\n",
    "\n",
    "    \n",
    "    def compute_loss(self, logits, attention_mask, labels):\n",
    "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "\n",
    "            active_logits = logits.view(-1, self.num_iob_classes)\n",
    "\n",
    "            active_loss_mask = attention_mask.view(-1) == 1\n",
    "            active_labels = torch.where(\n",
    "                active_loss_mask, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "            )\n",
    "        else:\n",
    "            num_classes = logits.shape[-1]\n",
    "            active_logits = logits.view(-1, num_classes)\n",
    "            active_labels = labels.view(-1)\n",
    "\n",
    "        # computes the loss between labels and logits \n",
    "        loss = loss_fct(active_logits, active_labels)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "            \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\"Standard `fit` method.\n",
    "        \n",
    "        fit() expects embeddings in X and strings in y.\n",
    "        The class itself is in charge of encoding the labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : [embeddings, attention_mask]\n",
    "        y : array-like, a list of lists of string [['O', 'B-fromcity']]\n",
    "        kwargs : dict\n",
    "            For passing other parameters. If 'X_dev' is included,\n",
    "            then performance is monitored every 10 epochs; use\n",
    "            `dev_iter` to control this number.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "\n",
    "        \"\"\"               \n",
    "        \n",
    "        ################################################################\n",
    "        # Model definition\n",
    "        ################################################################\n",
    "        \n",
    "        # Graph:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            self.compute_class2index(y)  # expects strings, must run before tensorizing y\n",
    "            self.model = self.define_graph()\n",
    "        \n",
    "        # Prime the model for training\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        \n",
    "        # Default is torch.optim.Adam\n",
    "        optimizer = self.optimizer(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.l2_strength)    \n",
    "          \n",
    "        \n",
    "        ################################################################\n",
    "        # Data \n",
    "        ################################################################\n",
    "        # separate the feature vectors (embeddings) from the attention_mask\n",
    "        X, attention_mask = X\n",
    "#         print(f\"X: {type(X)},     attention_mask: {type(attention_mask)},    y: [{len(y)}, {len(y[0])}]\")\n",
    "         \n",
    "        # Compute Incremental performance:\n",
    "        X_dev = kwargs.get('X_dev')\n",
    "        if X_dev is not None:\n",
    "            # X_dev contains 2 parameters, X_dev, attention_mask_dev\n",
    "            # the dev mask stays as numpy as is not used for anything\n",
    "            # because the class does not compute dev loss, it just stores \n",
    "            # the dev predictions. This mask is required by predict but\n",
    "            # it is just returned in the predict results (never used)            \n",
    "            dev_iter = kwargs.get('dev_iter', 10)\n",
    "            \n",
    "            \n",
    "        # separate the token labels from the sentence label\n",
    "        y_iob, y_intent = y\n",
    "\n",
    "        # encode labels (label vectorization). must run before tensorizing y\n",
    "        y_iob = self.encode_iob_labels(y_iob)\n",
    "        y_iob = self.pad_to_max_length(y_iob, X.shape[:2]) # y must have the shape of first 2 dims of X\n",
    "        y_intent = self.encode_intent_labels(y_intent)\n",
    "            \n",
    "        # cast data into PyTorch tensors\n",
    "        X = torch.tensor(X)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.bool)\n",
    "        y_iob = torch.tensor(y_iob, dtype=torch.long)\n",
    "        y_intent = torch.tensor(y_intent, dtype=torch.long)\n",
    "\n",
    "        # Wrap data into a dataset and use a Dataloader for batching\n",
    "        dataset = torch.utils.data.TensorDataset(X, attention_mask, y_iob, y_intent)\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, \n",
    "            shuffle=True, pin_memory=True) \n",
    "        \n",
    "\n",
    "        ################################################################\n",
    "        # Training process (Gradient Descent)\n",
    "        ################################################################\n",
    "        for iteration in range(1, self.config[\"max_iter\"]+1):\n",
    "            epoch_error = 0.0\n",
    "            for i, (X_batch, m_batch, y_iob_batch, y_intent_batch) in enumerate(dataloader):\n",
    "                # load the batch input tensors into GPU memory\n",
    "                X_batch = X_batch.to(self.device, non_blocking=True)\n",
    "                m_batch = m_batch.to(self.device, non_blocking=True)\n",
    "                \n",
    "                # call forward (mask is not used unless you want to compute the training loss)\n",
    "                logits_iob, logits_intent = self.model.forward(X=[X_batch, m_batch])\n",
    "\n",
    "                # load the batch label tensors into GPU memory\n",
    "                y_iob_batch = y_iob_batch.to(self.device, non_blocking=True)\n",
    "                y_intent_batch = y_intent_batch.to(self.device, non_blocking=True)\n",
    "                \n",
    "                # compute the loss, the gradients and update the weights\n",
    "                err_iob = self.compute_loss(logits_iob, m_batch, y_iob_batch)\n",
    "                err_intent = self.compute_loss(logits_intent, None, y_intent_batch)\n",
    "                err = err_iob + err_intent\n",
    "                epoch_error += err.item()\n",
    "                # backprop pass\n",
    "                optimizer.zero_grad()\n",
    "                err.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Incremental predictions where possible:\n",
    "            if X_dev is not None and iteration > 0 and iteration % dev_iter == 0:\n",
    "                self.dev_predictions[iteration] = self.predict(X_dev)\n",
    "                self.model.train()\n",
    "            self.errors.append(epoch_error)\n",
    "            progress_bar(\n",
    "                \"Finished epoch {} of {}; error is {}\".format(\n",
    "                    iteration, self.config[\"max_iter\"], epoch_error))\n",
    "        return self\n",
    "\n",
    "\n",
    "    \n",
    "    def predict_flat(self, X):\n",
    "        \"\"\"Predicted classes for the examples in `X`. In flat format for metric functions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "        attention_mask: input mask\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probs: torch.tensor(batch_size * max_sequence_length, num_classes)\n",
    "        pred_class: numpy.array(batch_size * max_sequence_length)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        predict_output = self.predict(X)\n",
    "        \n",
    "        probs_iob, preds_iob = predict_output[\"probs_iob\"], predict_output[\"preds_iob\"]\n",
    "        \n",
    "        # Flatten and apply mask        \n",
    "        probs_iob_flat = probs_iob.flatten()\n",
    "        probs_iob_flat = probs_iob_flat[attention_mask.flatten() == 1]\n",
    "        \n",
    "        preds_iob_flat = preds_iob.flatten()\n",
    "        preds_iob_flat = preds_iob_flat[attention_mask.flatten() == 1]        \n",
    "        \n",
    "        return {\"probs_iob_flat\": probs_iob_flat, \n",
    "                \"preds_iob_flat\": preds_iob_flat, \n",
    "                \"probs_intent\": probs_intent, \n",
    "                \"preds_intent\": preds_intent, \n",
    "                \"attention_mask\": attention_mask}\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicted classes for the examples in `X`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "        attention_mask: np.array \n",
    "            input mask\n",
    "            # the dev mask stays as numpy as is not used for anything\n",
    "            # because the class does not compute dev loss, it just stores \n",
    "            # the dev predictions. This mask is required by predict but\n",
    "            # it is just returned in the predict results (never used)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probs: torch.tensor(batch_size, max_sequence_length, num_classes)\n",
    "        pred_class: numpy.array(batch_size, max_sequence_length)\n",
    "        attention_mask\n",
    "\n",
    "        \"\"\"\n",
    "        # get the attention_mask to return it\n",
    "        _, attention_mask = X\n",
    "\n",
    "        # compute probabilities and predicted class\n",
    "        probs_iob, probs_intent = self.predict_proba(X)\n",
    "\n",
    "        # compute predicted class, maximizing across the last dimension (classes)\n",
    "        _, pred_iob_class_idx = torch.max(probs_iob, dim=-1)\n",
    "        pred_iob_class_idx = pred_iob_class_idx.detach().cpu().numpy()\n",
    "        \n",
    "        _, pred_intent_class_idx = torch.max(probs_intent, dim=-1)\n",
    "        pred_intent_class_idx = pred_intent_class_idx.detach().cpu().numpy()\n",
    "        \n",
    "        # decode the class indices to class names (IOB_tag)\n",
    "        preds_iob = [[self.iob_index2class[i] for i in row_class_idx] \n",
    "                              for row_class_idx in pred_iob_class_idx]\n",
    "        preds_iob = np.array(preds_iob)\n",
    "\n",
    "        preds_intent = [self.intent_index2class[i] for i in pred_intent_class_idx]\n",
    "        preds_iob = np.array(preds_iob)\n",
    "        \n",
    "        # detach prob tensors\n",
    "        probs_iob = probs_iob.detach().cpu().numpy()\n",
    "        probs_intent = probs_intent.detach().cpu().numpy()\n",
    "\n",
    "        return {\"probs_iob\": probs_iob, \n",
    "                \"preds_iob\": preds_iob, \n",
    "                \"probs_intent\": probs_intent, \n",
    "                \"preds_intent\": preds_intent, \n",
    "                \"attention_mask\": attention_mask}\n",
    "        \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predicted probabilities for the examples in `X`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor(batch_size, max_sequence_length, num_classes)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Graph:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            # self.class2index must be defined in this case\n",
    "            self.model = self.define_graph()\n",
    "        \n",
    "        # prime the model for prediction-only mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            self.model.to(self.device)\n",
    "       \n",
    "            # cast input data into PyTorch tensors\n",
    "            x, attention_mask = X\n",
    "            x, attention_mask = torch.tensor(x), torch.tensor(attention_mask, dtype=torch.bool)\n",
    "\n",
    "            # load the input tensors into GPU memory\n",
    "            x = x.to(self.device)\n",
    "            attention_mask = attention_mask.to(self.device)\n",
    "            \n",
    "            # call forward \n",
    "            # (mask is not used unless you want to compute the training loss)\n",
    "            logits_iob, logits_intent = self.model.forward(X=[x, attention_mask])\n",
    " \n",
    "            # compute probabilities and predicted class\n",
    "            probs_iob = nn.Softmax(dim=-1)(logits_iob) # normalize scores along the latest dimension\n",
    "            probs_intent = nn.Softmax(dim=-1)(logits_intent) # normalize scores along the latest dimension\n",
    "            \n",
    "            return probs_iob, probs_intent  # tensor (no_grad)\n",
    "        \n",
    "    def compute_class2index(self, y):\n",
    "        \"\"\"\n",
    "        y: 2-D list of (lists of) strings (iob_tags, not indices)\n",
    "        \n",
    "        expects strings, must run before tensorizing y,\n",
    "        must run before defining the graph because it computes\n",
    "        the output network output (num_classes)\n",
    "        \n",
    "        Note:\n",
    "        if the input type is incorrect and it results in a number of classes is incorrect (very high) \n",
    "        it will likely cause a CUDA-OUT-OF-MEMORY error\n",
    "        \"\"\"\n",
    "        # separate the token labels from the sentence label\n",
    "        y_iob, y_intent = y\n",
    "        \n",
    "        # flat list of iob labels\n",
    "        iob_labels = []\n",
    "        for y_row in y_iob:\n",
    "            iob_labels.extend(y_row)\n",
    "            \n",
    "        # create iob mapping\n",
    "        iob_classes = sorted(set(iob_labels))\n",
    "        self.iob_class2index = dict(zip(iob_classes, range(len(iob_classes))))\n",
    "        self.iob_index2class = {i:c for c, i in self.iob_class2index.items()}        \n",
    "\n",
    "        # create intent mapping\n",
    "        intent_classes = sorted(set(y_intent))\n",
    "        self.intent_class2index = dict(zip(intent_classes, range(len(intent_classes))))\n",
    "        self.intent_index2class = {i:c for c, i in self.intent_class2index.items()}        \n",
    "\n",
    "        \n",
    "    def encode_intent_labels(self, y_intent):\n",
    "        \"\"\"\n",
    "        y: 1-D list of (lists of) strings (intent)\n",
    "        \n",
    "        expects strings, must run before tensorizing y,\n",
    "        \"\"\"\n",
    "\n",
    "        return  [self.intent_class2index[label] for label in y_intent]\n",
    "        \n",
    "        \n",
    "    def encode_iob_labels(self, y):\n",
    "        \"\"\"\n",
    "        y: 2-D list of (lists of) strings (iob_tags, not indices)\n",
    "        \n",
    "        expects strings, must run before tensorizing y,\n",
    "        \"\"\"\n",
    "        tag_id_matrix = []\n",
    "        for iob_tags in y:\n",
    "            tag_ids = [self.iob_class2index[iob_tag] for iob_tag in iob_tags]\n",
    "            tag_id_matrix.append(tag_ids)  \n",
    "\n",
    "        return  tag_id_matrix\n",
    "    \n",
    "    def pad_to_max_length(self, jagged_matrix, output_shape):\n",
    "        padded_matrix = np.zeros(shape=output_shape)\n",
    "        for i, row in enumerate(jagged_matrix):\n",
    "            padded_matrix[i, :len(row)] = row \n",
    "        return padded_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Training and Prediction of the Shallow Slot Filling model (the Shallow model uses Bert embeddings without fine-tunning)\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling: Alignment, Encoding, Normalize the length of the sequences, Class Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(class_ids):\n",
    "    \"\"\"\n",
    "        class_ids: 1D tensor, contains one class_id for each example\n",
    "    \"\"\"\n",
    "    # encode the class_ids as onehot\n",
    "    class_matrix = np.zeros(shape=(len(class_ids), max(class_ids)))\n",
    "    class_matrix[class_ids] = 1\n",
    "    \n",
    "    # set the positive weights as the fraction of negative labels (0) for each class (each column)\n",
    "    w_p = np.sum(class_matrix == 0, axis=0) / class_matrix.shape[0]\n",
    "\n",
    "    # set the negative weights as the fraction of positive labels (1) for each class (each column)\n",
    "    w_n = np.sum(class_matrix == 1, axis=0) / class_matrix.shape[0]\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label and Sub-token Alignment (call WordPiece for each token, output word_to_tok_map and aligned_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_start_label_aligner(sentence, word_labels=None):\n",
    "    \"\"\"\n",
    "    Aligns the IOB labels to the word-starting tokens in the list of\n",
    "    sub-word tokens returned by the WordPiece tokenizer.\n",
    "    Returns:\n",
    "    - an array of indices, each pointing to the first sub-token of every word\n",
    "    - a padded list of labels, which has one element for each sub-token (the first\n",
    "      sub-token of every word gets the label, the rest get the padding label 'X')\n",
    "    \"\"\"\n",
    "    # Token map will be an int -> int mapping between the `word` index in the sentence and\n",
    "    # the WordPiece `tokens` index.\n",
    "    word_start_indices = []\n",
    "    tokens = [\"[CLS]\"]\n",
    "    if word_labels is not None:\n",
    "        token_labels = [\"O\"]\n",
    "    else:\n",
    "        token_labels = None\n",
    "    if len(sentence.split()) != len(word_labels):\n",
    "        print(f\"sentence: {len(sentence.split())}, word_labels: {len(word_labels)}\")\n",
    "        print(f\"sentence: {sentence.split()}, word_labels: {word_labels}\")\n",
    "    for word_idx, word in enumerate(sentence.split(' ')):\n",
    "        word_start_indices.append(len(tokens))\n",
    "        word_tokens = hf_tokenizer.tokenize(word)  # tokenize ONE word \n",
    "        tokens.extend(word_tokens)\n",
    "        if word_labels is not None:\n",
    "            token_labels.append(word_labels[word_idx])\n",
    "            if len(word_tokens) > 1:\n",
    "                token_labels.extend([\"X\"]*(len(word_tokens)-1))\n",
    "\n",
    "    tokens.append(\"[SEP]\")\n",
    "    token_labels.append( \"O\")\n",
    "    return token_labels, word_start_indices, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_label_aligner(sentences, labels):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    label_matrix, word_start_matrix, token_matrix = [], [], []\n",
    "    \n",
    "    for sentence, word_labels in zip(sentences, labels):\n",
    "        token_labels, word_start_indices, tokens =\\\n",
    "            word_start_label_aligner(sentence, word_labels)\n",
    "        \n",
    "        label_matrix.append(token_labels)\n",
    "        word_start_matrix.append(word_start_indices)\n",
    "        token_matrix.append(tokens)\n",
    "\n",
    "    return label_matrix, word_start_matrix, token_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_predict_output(y_true, preds, attention_mask):\n",
    "    \"\"\"\n",
    "    y_true : list of list of strings (IOB_tags)\n",
    "        y_true is a list of  variable-length lists of strings, \n",
    "        is token-alignmed but not lenght-padded\n",
    "        \n",
    "    preds : np.array(batch_size, max_sentence_length)\n",
    "        2-D array with the class predicted by the model,\n",
    "        for each token of each sentence.\n",
    "        The attention mask must be applied to exclude the padding tokens.\n",
    "        \n",
    "    attention_mask: np.array(batch_size, max_sentence_length)\n",
    "        boolean tensor to filter the padding tokens\n",
    "    \n",
    "    In order to produce a classification report for sequence tagging, \n",
    "    first al the arrays need to be flattened.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"len(y_true): {len(y_true)}\")\n",
    "    print(f\"preds.shape: {preds.shape}\")\n",
    "    print(f\"attention_mask.shape: {attention_mask.shape}\")\n",
    "\n",
    "    # flatten the sequence labels\n",
    "    y_flat = []\n",
    "    for iob_tags in y_true:\n",
    "        y_flat.extend(iob_tags)\n",
    "    print(f\"y_flat (flattened): {len(y_flat)}\")\n",
    "\n",
    "    # apply mask to remove padding token positions and flatten the matrix\n",
    "    preds_flat = preds.flatten()\n",
    "    print(f\"preds_flat.shape (flattened): {preds_flat.shape}\")\n",
    "    preds_flat = preds_flat[attention_mask.flatten() == 1]\n",
    "    print(f\"preds_flat.shape (masked): {preds_flat.shape}\")\n",
    "    \n",
    "    return y_flat, preds_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_classification_report(y, predict_output, digits=3):\n",
    "    \"\"\"\n",
    "    Adapts the interface between the experiment and the sequence-tagging report function\n",
    "    y : non-padded token_label_matrix\n",
    "        list of list of strings (IOB_tags)\n",
    "        y is a list of  variable-length lists of strings, \n",
    "        is token-alignmed but not lenght-padded\n",
    "        \n",
    "    preds : np.array(batch_size, max_sentence_length)\n",
    "        2-D array with the class predicted by the model,\n",
    "        for each token of each sentence.\n",
    "        The attention mask must be applied to exclude the padding tokens.\n",
    "        \n",
    "    attention_mask:\n",
    "        boolean tensor to filter the padding tokens\n",
    "    \"\"\"\n",
    "    probs, preds, attention_mask = predict_output[:3]\n",
    "    print(f\"cr-probs: {probs.shape}\")\n",
    "    print(f\"cr-preds: {preds.shape}\")\n",
    "    print(f\"cr-attention_mask: {attention_mask.shape}\")\n",
    "        \n",
    "    y_flat, preds_flat =\\\n",
    "        flatten_predict_output(y, preds, attention_mask)\n",
    "\n",
    "    print(classification_report(y_flat, preds_flat, digits=digits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tagging_macro_f1(y, predict_output, digits=3):\n",
    "    \"\"\"\n",
    "    Adapts the interface between the experiment and the sequence-tagging scoring function\n",
    "    y : non-padded token_label_matrix\n",
    "        list of list of strings (IOB_tags)\n",
    "        y is a list of  variable-length lists of strings, \n",
    "        is token-alignmed but not lenght-padded\n",
    "        \n",
    "        \n",
    "    preds : np.array(batch_size, max_sentence_length)\n",
    "        2-D array with the class predicted by the model,\n",
    "        for each token of each sentence.\n",
    "        The attention mask must be applied to exclude the padding tokens.\n",
    "        \n",
    "    attention_mask:\n",
    "        boolean tensor to filter the padding tokens\n",
    "    \"\"\"\n",
    "    probs, preds, attention_mask = predict_output[:3]\n",
    "    \n",
    "    y_flat, pred_flat =\\\n",
    "        flatten_predict_output(y, preds, attention_mask)\n",
    "    \n",
    "    return utils.safe_macro_f1(y_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Shallow SLU Joint Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_shallow_slu_joint_model(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    X_dev, \n",
    "    y_dev):\n",
    "    \"\"\"\n",
    "    fit() expects in X and strings in y.\n",
    "    The class itself is in charge of encoding (a.k.a. 'vectorizing') the labels.\n",
    "    \n",
    "    unit test the TorchShallowSequenceTagger.fit() method \n",
    "    This test calls the fit method on an untrained model \n",
    "    and later calls the metrics report on the dev results of the trained model\n",
    "    \"\"\"\n",
    "\n",
    "    # configure the sequence tagging layer\n",
    "    joint_config = {\n",
    "        \"input_embedding_size\": 768,\n",
    "        \"hidden_dropout_prob\": 0.4,\n",
    "        \"objective_weights\": None,  # loss penalizes more SF or ID\n",
    "        \"class_weights\": None,\n",
    "        \"batch_size\": 32,\n",
    "        \"lr\": 1e-3,\n",
    "        \"l2_strength\": 0,\n",
    "        \"max_iter\": 10,\n",
    "        \"device\": \"cuda\"\n",
    "    }\n",
    "    \n",
    "    # Shallow network\n",
    "    joint_model = TorchJointSlu(joint_config)   \n",
    "\n",
    "    # unpack train data\n",
    "    X_train, m_train = X_train\n",
    "    y_iob_tags_matrix_train, y_intent_train = y_train\n",
    "\n",
    "    # unpack eval data\n",
    "    X_dev, m_dev = X_dev\n",
    "    y_iob_tags_matrix_dev, y_intent_dev = y_dev\n",
    "    \n",
    "    # Fit\n",
    "    # we need to pass embeddings in X and strings in y\n",
    "    joint_model.fit(\n",
    "        X=[X_train, m_train],\n",
    "        y=[y_iob_tags_matrix_train, y_intent_train],\n",
    "        X_dev=[X_dev, m_dev],\n",
    "        y_dev=[y_iob_tags_matrix_dev, y_intent_dev]\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    predict_output = joint_model.predict(\n",
    "                        X=[X_dev, m_dev])\n",
    "    \n",
    "    probs_iob = predict_output[\"probs_iob\"]\n",
    "    preds_iob = predict_output[\"preds_iob\"]\n",
    "    attention_mask = predict_output[\"attention_mask\"]\n",
    "    \n",
    "    sequence_tagging_classification_report(y_iob_tags_matrix_dev, (probs_iob, preds_iob, attention_mask))\n",
    "    print(\"SF NON-VERBOSE MACRO-F1:\", sequence_tagging_macro_f1(y_iob_tags_matrix_dev, \n",
    "                                                             (probs_iob, preds_iob, attention_mask)))\n",
    "    \n",
    "     \n",
    "    print(\"\\n\\n\\nINTENT:\")\n",
    "    print(classification_report(y_intent_dev, predict_output[\"preds_intent\"]))\n",
    "    print(\"INTENT NON-VERBOSE MACRO-F1:\", utils.safe_macro_f1(y_intent_dev,  predict_output[\"preds_intent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    # read ATIS\n",
    "    atis_train = list(atis.train_reader(ATIS_HOME, class_func=atis.slot_filling_and_intent_func))\n",
    "    atis_dev = list(atis.dev_reader(ATIS_HOME, class_func=atis.slot_filling_and_intent_func))\n",
    "\n",
    "    # Split sentence and label\n",
    "    X_atis_sentences_train, y_atis_train = zip(*atis_train)\n",
    "    X_atis_sentences_dev, y_atis_dev = zip(*atis_dev)\n",
    "\n",
    "    # Split label iob and intent\n",
    "    y_iob_tags_train, y_intent_train = zip(*y_atis_train)\n",
    "    y_iob_tags_dev, y_intent_dev = zip(*y_atis_dev)\n",
    "\n",
    "    # Get token-level representations for all the input rows in the dataset\n",
    "    final_hidden_states_train, input_mask_train, input_token_ids_train =\\\n",
    "    batch_encoder_vectorizer(X_atis_sentences_train)\n",
    "\n",
    "    final_hidden_states_dev, input_mask_dev, input_token_ids_dev =\\\n",
    "    batch_encoder_vectorizer(X_atis_sentences_dev)\n",
    "\n",
    "    # align the labels to the sub-word tokens\n",
    "    y_iob_tags_matrix_train, _, _ =  sequence_tagging_label_aligner(X_atis_sentences_train, y_iob_tags_train)\n",
    "    y_iob_tags_matrix_dev, _, _ =  sequence_tagging_label_aligner(X_atis_sentences_dev, y_iob_tags_dev)\n",
    "    \n",
    "    # package data\n",
    "    X_train = (final_hidden_states_train, input_mask_train)\n",
    "    X_dev = (final_hidden_states_dev, input_mask_dev)\n",
    "    \n",
    "    y_train = (y_iob_tags_matrix_train, y_intent_train)\n",
    "    y_dev = (y_iob_tags_matrix_dev, y_intent_dev)\n",
    "    \n",
    "    return X_train, X_dev, y_train, y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_shallow_slu_joint_model():\n",
    "    X_train, X_dev, y_train, y_dev = read_dataset()\n",
    "\n",
    "    evaluate_shallow_slu_joint_model( X_train, y_train,\n",
    "                                      X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define_graph: num_iob_classes: 121\n",
      "define_graph: num_intent_classes: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 10 of 10; error is 50.60976545512676"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cr-probs: (500, 47, 121)\n",
      "cr-preds: (500, 47)\n",
      "cr-attention_mask: (500, 47)\n",
      "len(y_true): 500\n",
      "preds.shape: (500, 47)\n",
      "attention_mask.shape: (500, 47)\n",
      "y_flat (flattened): 9348\n",
      "preds_flat.shape (flattened): (23500,)\n",
      "preds_flat.shape (masked): (9348,)\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "             B-aircraft_code      0.000     0.000     0.000         1\n",
      "              B-airline_code      1.000     0.667     0.800         9\n",
      "              B-airline_name      0.968     0.968     0.968        62\n",
      "              B-airport_code      1.000     0.750     0.857         4\n",
      "              B-airport_name      0.000     0.000     0.000         4\n",
      " B-arrive_date.date_relative      0.000     0.000     0.000         2\n",
      "      B-arrive_date.day_name      0.000     0.000     0.000        10\n",
      "    B-arrive_date.day_number      0.000     0.000     0.000         4\n",
      "    B-arrive_date.month_name      0.000     0.000     0.000         4\n",
      "B-arrive_date.today_relative      0.000     0.000     0.000         1\n",
      "      B-arrive_time.end_time      1.000     0.667     0.800         3\n",
      "    B-arrive_time.period_mod      0.000     0.000     0.000         1\n",
      " B-arrive_time.period_of_day      1.000     0.077     0.143        13\n",
      "    B-arrive_time.start_time      1.000     0.667     0.800         3\n",
      "          B-arrive_time.time      0.526     0.588     0.556        17\n",
      " B-arrive_time.time_relative      0.812     0.929     0.867        14\n",
      "                 B-city_name      0.800     0.696     0.744        23\n",
      "                B-class_type      1.000     0.957     0.978        23\n",
      "                   B-connect      0.667     0.500     0.571         4\n",
      "             B-cost_relative      0.971     1.000     0.985        33\n",
      "                  B-day_name      0.000     0.000     0.000         2\n",
      "                 B-days_code      1.000     1.000     1.000         1\n",
      " B-depart_date.date_relative      0.375     0.500     0.429         6\n",
      "      B-depart_date.day_name      0.881     1.000     0.937       104\n",
      "    B-depart_date.day_number      0.905     0.950     0.927        40\n",
      "    B-depart_date.month_name      0.902     0.925     0.914        40\n",
      "B-depart_date.today_relative      1.000     1.000     1.000         9\n",
      "          B-depart_date.year      1.000     1.000     1.000         4\n",
      "      B-depart_time.end_time      1.000     1.000     1.000         1\n",
      "    B-depart_time.period_mod      0.800     0.889     0.842         9\n",
      " B-depart_time.period_of_day      0.800     0.944     0.866        72\n",
      "    B-depart_time.start_time      0.500     1.000     0.667         1\n",
      "          B-depart_time.time      0.791     0.810     0.800        42\n",
      " B-depart_time.time_relative      0.944     0.850     0.895        40\n",
      "                   B-economy      1.000     1.000     1.000         2\n",
      "               B-fare_amount      1.000     0.714     0.833         7\n",
      "           B-fare_basis_code      1.000     0.923     0.960        13\n",
      "               B-flight_days      1.000     0.500     0.667         2\n",
      "                B-flight_mod      0.872     0.850     0.861        40\n",
      "             B-flight_number      1.000     0.833     0.909         6\n",
      "               B-flight_stop      1.000     0.920     0.958        25\n",
      "               B-flight_time      1.000     0.500     0.667        12\n",
      "      B-fromloc.airport_code      1.000     1.000     1.000         1\n",
      "      B-fromloc.airport_name      0.800     0.250     0.381        16\n",
      "         B-fromloc.city_name      0.920     0.949     0.934       434\n",
      "        B-fromloc.state_code      1.000     1.000     1.000         5\n",
      "        B-fromloc.state_name      0.667     0.500     0.571         4\n",
      "                      B-meal      1.000     1.000     1.000         3\n",
      "          B-meal_description      1.000     1.000     1.000         8\n",
      "                       B-mod      0.000     0.000     0.000         1\n",
      "                        B-or      1.000     0.571     0.727         7\n",
      "             B-period_of_day      0.000     0.000     0.000         2\n",
      "          B-restriction_code      1.000     1.000     1.000         2\n",
      " B-return_date.date_relative      1.000     0.333     0.500         3\n",
      "    B-return_date.day_number      0.000     0.000     0.000         2\n",
      "    B-return_date.month_name      0.000     0.000     0.000         2\n",
      " B-return_time.period_of_day      0.000     0.000     0.000         1\n",
      "                B-round_trip      1.000     1.000     1.000        25\n",
      "                B-state_code      0.000     0.000     0.000         1\n",
      "                B-state_name      0.000     0.000     0.000         1\n",
      "         B-stoploc.city_name      0.833     0.714     0.769        21\n",
      "        B-toloc.airport_code      1.000     0.500     0.667         4\n",
      "        B-toloc.airport_name      0.500     0.500     0.500         4\n",
      "           B-toloc.city_name      0.933     0.950     0.942       424\n",
      "          B-toloc.state_code      0.909     1.000     0.952        10\n",
      "          B-toloc.state_name      0.667     0.571     0.615         7\n",
      "            B-transport_type      1.000     1.000     1.000         8\n",
      "              I-airline_name      1.000     0.952     0.976        42\n",
      "              I-airport_name      0.667     1.000     0.800         4\n",
      "      I-arrive_time.end_time      1.000     0.333     0.500         3\n",
      "          I-arrive_time.time      0.875     0.583     0.700        12\n",
      "                 I-city_name      1.000     0.250     0.400         4\n",
      "                I-class_type      1.000     1.000     1.000        19\n",
      "             I-cost_relative      1.000     1.000     1.000         6\n",
      "    I-depart_date.day_number      0.929     1.000     0.963        13\n",
      "      I-depart_time.end_time      1.000     1.000     1.000         1\n",
      " I-depart_time.period_of_day      0.000     0.000     0.000         1\n",
      "    I-depart_time.start_time      1.000     1.000     1.000         1\n",
      "          I-depart_time.time      0.850     0.971     0.907        35\n",
      "               I-fare_amount      1.000     1.000     1.000         6\n",
      "           I-fare_basis_code      0.000     0.000     0.000         1\n",
      "                I-flight_mod      0.000     0.000     0.000         2\n",
      "               I-flight_stop      1.000     1.000     1.000         2\n",
      "               I-flight_time      1.000     0.667     0.800         6\n",
      "      I-fromloc.airport_name      1.000     0.455     0.625        22\n",
      "         I-fromloc.city_name      0.917     0.932     0.924        59\n",
      "        I-fromloc.state_name      0.000     0.000     0.000         1\n",
      "          I-restriction_code      1.000     1.000     1.000         1\n",
      "    I-return_date.day_number      0.000     0.000     0.000         1\n",
      "                I-round_trip      1.000     1.000     1.000        25\n",
      "         I-stoploc.city_name      0.500     0.167     0.250         6\n",
      "        I-toloc.airport_name      0.250     0.333     0.286         3\n",
      "           I-toloc.city_name      0.934     0.884     0.908       112\n",
      "          I-toloc.state_name      0.000     0.000     0.000         1\n",
      "            I-transport_type      1.000     1.000     1.000         2\n",
      "                           O      0.985     0.997     0.991      4603\n",
      "                           X      0.990     0.996     0.993      2645\n",
      "\n",
      "                    accuracy                          0.970      9348\n",
      "                   macro avg      0.697     0.618     0.637      9348\n",
      "                weighted avg      0.966     0.970     0.966      9348\n",
      "\n",
      "len(y_true): 500\n",
      "preds.shape: (500, 47)\n",
      "attention_mask.shape: (500, 47)\n",
      "y_flat (flattened): 9348\n",
      "preds_flat.shape (flattened): (23500,)\n",
      "preds_flat.shape (masked): (9348,)\n",
      "SF NON-VERBOSE MACRO-F1: 0.6369179201423336\n",
      "\n",
      "\n",
      "\n",
      "INTENT:\n",
      "                               precision    recall  f1-score   support\n",
      "\n",
      "            atis_abbreviation       0.84      0.94      0.89        17\n",
      "                atis_aircraft       0.75      0.55      0.63        11\n",
      "                 atis_airfare       0.77      0.87      0.81        38\n",
      "atis_airfare#atis_flight_time       0.00      0.00      0.00         1\n",
      "                 atis_airline       1.00      0.39      0.56        18\n",
      "                 atis_airport       1.00      0.33      0.50         3\n",
      "                atis_capacity       1.00      1.00      1.00         1\n",
      "                    atis_city       1.00      1.00      1.00         1\n",
      "                atis_distance       1.00      0.33      0.50         3\n",
      "                  atis_flight       0.90      0.98      0.94       357\n",
      "     atis_flight#atis_airfare       0.00      0.00      0.00         2\n",
      "             atis_flight_time       0.00      0.00      0.00         9\n",
      "             atis_ground_fare       1.00      0.33      0.50         3\n",
      "          atis_ground_service       0.95      0.84      0.89        25\n",
      "                atis_quantity       1.00      0.90      0.95        10\n",
      "             atis_restriction       0.00      0.00      0.00         1\n",
      "\n",
      "                     accuracy                           0.89       500\n",
      "                    macro avg       0.70      0.53      0.57       500\n",
      "                 weighted avg       0.87      0.89      0.87       500\n",
      "\n",
      "INTENT NON-VERBOSE MACRO-F1: 0.5735705192784447\n"
     ]
    }
   ],
   "source": [
    "experiment_shallow_slu_joint_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertJointSluModel(nn.Module):\n",
    "    def __init__(self,\n",
    "            output_iob_dim,\n",
    "            output_intent_dim,\n",
    "            dropout_prob,\n",
    "            weights_name='bert-base-cased'):\n",
    "        super(BertJointSluModel, self).__init__()\n",
    "        \n",
    "        self.weights_name = weights_name\n",
    "        self.output_iob_dim = output_iob_dim\n",
    "        self.output_intent_dim = output_intent_dim\n",
    "        \n",
    "        # Graph\n",
    "        self.bert = BertModel.from_pretrained(self.weights_name)\n",
    "        self.embed_dim = self.bert.embeddings.word_embeddings.embedding_dim \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob) \n",
    "        \n",
    "        self.iob_classifier_layer = nn.Linear(self.embed_dim, output_iob_dim)\n",
    "        self.init_weights(self.iob_classifier_layer) \n",
    "\n",
    "        self.intent_classifier_layer = nn.Linear(self.embed_dim, output_intent_dim)\n",
    "        self.init_weights(self.intent_classifier_layer) \n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        torch.nn.init.xavier_uniform(layer.weight) \n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.zero_()        \n",
    "                    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Here, `X` is a list of two np.array  \n",
    "        consisting of the token_ids (an index into the BERT embedding)\n",
    "        and the attention_mask (a 1 or 0 indicating whether the token \n",
    "        is masked). The `fit` method will \n",
    "        train all these parameters against a softmax objective.\n",
    "        \n",
    "        \"\"\"\n",
    "        # separates the indices from the mask\n",
    "        indices, mask = X\n",
    "        # Type conversion, since the base class insists on\n",
    "        # casting this as a FloatTensor, but we ned Long\n",
    "        # for `bert`.\n",
    "        indices = indices.long()\n",
    "        \n",
    "        # graph execution\n",
    "        final_hidden_states, cls_output =\\\n",
    "            self.bert(indices, attention_mask=mask)\n",
    "        \n",
    "        h = self.dropout(final_hidden_states)\n",
    "        \n",
    "        logits_iob = self.iob_classifier_layer(h)\n",
    "        \n",
    "        # create single embedding per sentence\n",
    "        sentence_h = h.mean(axis=1)  # for better performance\n",
    "        \n",
    "        logits_intent = self.intent_classifier_layer(sentence_h)\n",
    "\n",
    "        return logits_iob, logits_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertJointSlu(TorchJointSlu):\n",
    "    def __init__(self, weights_name, *args, **kwargs):\n",
    "        self.weights_name = weights_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.weights_name)\n",
    "        super(BertJointSlu, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def define_graph(self):\n",
    "        \"\"\"This method is used by `fit`. We override it here to use our\n",
    "        new BERT-based graph.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.num_iob_classes = len(self.iob_class2index)   # class2index is set in fit()\n",
    "        self.num_intent_classes = len(self.intent_class2index)   # class2index is set in fit()\n",
    "        print(f\"define_graph: num_iob_classes: {self.num_iob_classes}\")\n",
    "        print(f\"define_graph: num_intent_classes: {self.num_intent_classes}\")\n",
    "        model = BertJointSluModel(\n",
    "            output_iob_dim=self.num_iob_classes, \n",
    "            output_intent_dim=self.num_intent_classes, \n",
    "            dropout_prob=self.hidden_dropout_prob, \n",
    "            weights_name=self.weights_name)\n",
    "        model.train() # flag\n",
    "        return model\n",
    "    \n",
    "    def encode(self, X, max_length=None):\n",
    "        \"\"\"The `X` is a list of strings. We use the model's tokenizer\n",
    "        to get the indices and mask information.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list of [index, mask] pairs, where index is an int and mask\n",
    "        is 0 or 1.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ## IN FINE TUNNIG WE DEAL WITH TOKEN_IDS (indices)\n",
    "        \n",
    "        data = self.tokenizer.batch_encode_plus(\n",
    "            X, \n",
    "            max_length=max_length,\n",
    "            add_special_tokens=True, \n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True)\n",
    "        indices = np.array(data['input_ids'])\n",
    "        mask = np.array(data['attention_mask'])\n",
    "        \n",
    "        return [indices, mask]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a self-contained illustration, starting from the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_for_fine_tunning(hf_fine_tune_mod):\n",
    "    # read ATIS\n",
    "    atis_train = list(atis.train_reader(ATIS_HOME, class_func=atis.slot_filling_and_intent_func))\n",
    "    atis_dev = list(atis.dev_reader(ATIS_HOME, class_func=atis.slot_filling_and_intent_func))\n",
    "\n",
    "    # Split sentence and label\n",
    "    X_atis_sentences_train, y_atis_train = zip(*atis_train)\n",
    "    X_atis_sentences_dev, y_atis_dev = zip(*atis_dev)\n",
    "\n",
    "    # Split label iob and intent\n",
    "    y_iob_tags_train, y_intent_train = zip(*y_atis_train)\n",
    "    y_iob_tags_dev, y_intent_dev = zip(*y_atis_dev)\n",
    "\n",
    "    # align the labels to the sub-word tokens\n",
    "    y_iob_tags_matrix_train, _, _ =  sequence_tagging_label_aligner(X_atis_sentences_train, y_iob_tags_train)\n",
    "    y_iob_tags_matrix_dev, _, _ =  sequence_tagging_label_aligner(X_atis_sentences_dev, y_iob_tags_dev)\n",
    "    \n",
    "    # encode the inputs\n",
    "    X_indices_mask_train = hf_fine_tune_mod.encode(X_atis_sentences_train)\n",
    "    X_indices_mask_dev = hf_fine_tune_mod.encode(X_atis_sentences_dev)  \n",
    "\n",
    "    # package data\n",
    "    X_train = X_indices_mask_train\n",
    "    X_dev = X_indices_mask_dev\n",
    "    \n",
    "    y_train = (y_iob_tags_matrix_train, y_intent_train)\n",
    "    y_dev = (y_iob_tags_matrix_dev, y_intent_dev)\n",
    "    \n",
    "    return X_train, X_dev, y_train, y_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has some standard fine-tuning parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configures the sequence tagging layer\n",
    "joint_slu_config = {\n",
    "    \"input_embedding_size\": None, # not needed for fine-tunning\n",
    "    \"hidden_dropout_prob\": 0.4,\n",
    "    \"class_weights\": None,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 0.0002,   # eta\n",
    "    \"l2_strength\": 0,\n",
    "    \"max_iter\": 8,   # keep small during debug\n",
    "    \"device\": \"cuda\"\n",
    "}\n",
    "\n",
    "\n",
    "hf_fine_tune_mod = BertJointSlu(\n",
    "    'bert-base-cased', \n",
    "    config=joint_slu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev =\\\n",
    "    read_dataset_for_fine_tunning(hf_fine_tune_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define_graph: num_iob_classes: 121\n",
      "define_graph: num_intent_classes: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 8 of 8; error is 31.448452878743413"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 23s, sys: 2min 26s, total: 11min 49s\n",
      "Wall time: 11min 48s\n"
     ]
    }
   ],
   "source": [
    "%time _ = hf_fine_tune_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some predictions on the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_output = hf_fine_tune_mod.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cr-probs: (500, 47, 121)\n",
      "cr-preds: (500, 47)\n",
      "cr-attention_mask: (500, 47)\n",
      "len(y_true): 500\n",
      "preds.shape: (500, 47)\n",
      "attention_mask.shape: (500, 47)\n",
      "y_flat (flattened): 9348\n",
      "preds_flat.shape (flattened): (23500,)\n",
      "preds_flat.shape (masked): (9348,)\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "             B-aircraft_code      1.000     1.000     1.000         1\n",
      "              B-airline_code      0.778     0.778     0.778         9\n",
      "              B-airline_name      0.984     0.984     0.984        62\n",
      "              B-airport_code      1.000     0.750     0.857         4\n",
      "              B-airport_name      0.333     0.500     0.400         4\n",
      " B-arrive_date.date_relative      0.333     0.500     0.400         2\n",
      "      B-arrive_date.day_name      0.857     0.600     0.706        10\n",
      "    B-arrive_date.day_number      1.000     0.500     0.667         4\n",
      "    B-arrive_date.month_name      1.000     1.000     1.000         4\n",
      "B-arrive_date.today_relative      0.000     0.000     0.000         1\n",
      "      B-arrive_time.end_time      1.000     1.000     1.000         3\n",
      "    B-arrive_time.period_mod      0.000     0.000     0.000         1\n",
      " B-arrive_time.period_of_day      1.000     0.385     0.556        13\n",
      "    B-arrive_time.start_time      1.000     0.667     0.800         3\n",
      "          B-arrive_time.time      0.933     0.824     0.875        17\n",
      " B-arrive_time.time_relative      1.000     0.857     0.923        14\n",
      "                 B-city_name      0.800     0.870     0.833        23\n",
      "                B-class_type      0.920     1.000     0.958        23\n",
      "                   B-connect      0.800     1.000     0.889         4\n",
      "             B-cost_relative      1.000     1.000     1.000        33\n",
      "                  B-day_name      1.000     1.000     1.000         2\n",
      "                 B-days_code      0.000     0.000     0.000         1\n",
      " B-depart_date.date_relative      0.429     0.500     0.462         6\n",
      "      B-depart_date.day_name      0.963     0.990     0.976       104\n",
      "    B-depart_date.day_number      0.902     0.925     0.914        40\n",
      "    B-depart_date.month_name      0.907     0.975     0.940        40\n",
      "B-depart_date.today_relative      1.000     1.000     1.000         9\n",
      "          B-depart_date.year      1.000     1.000     1.000         4\n",
      "      B-depart_time.end_time      0.500     1.000     0.667         1\n",
      "    B-depart_time.period_mod      0.800     0.889     0.842         9\n",
      " B-depart_time.period_of_day      0.855     0.986     0.916        72\n",
      "    B-depart_time.start_time      1.000     1.000     1.000         1\n",
      "          B-depart_time.time      0.875     1.000     0.933        42\n",
      " B-depart_time.time_relative      0.907     0.975     0.940        40\n",
      "                   B-economy      1.000     1.000     1.000         2\n",
      "               B-fare_amount      1.000     0.857     0.923         7\n",
      "           B-fare_basis_code      0.812     1.000     0.897        13\n",
      "               B-flight_days      1.000     1.000     1.000         2\n",
      "                B-flight_mod      0.974     0.925     0.949        40\n",
      "             B-flight_number      1.000     0.667     0.800         6\n",
      "               B-flight_stop      1.000     0.760     0.864        25\n",
      "               B-flight_time      0.421     0.667     0.516        12\n",
      "      B-fromloc.airport_code      0.500     1.000     0.667         1\n",
      "      B-fromloc.airport_name      1.000     0.625     0.769        16\n",
      "         B-fromloc.city_name      0.991     0.979     0.985       434\n",
      "        B-fromloc.state_code      1.000     1.000     1.000         5\n",
      "        B-fromloc.state_name      0.800     1.000     0.889         4\n",
      "                      B-meal      1.000     1.000     1.000         3\n",
      "                 B-meal_code      0.000     0.000     0.000         0\n",
      "          B-meal_description      1.000     1.000     1.000         8\n",
      "                       B-mod      0.000     0.000     0.000         1\n",
      "                        B-or      0.750     0.857     0.800         7\n",
      "             B-period_of_day      1.000     1.000     1.000         2\n",
      "          B-restriction_code      1.000     1.000     1.000         2\n",
      " B-return_date.date_relative      0.000     0.000     0.000         3\n",
      "    B-return_date.day_number      0.000     0.000     0.000         2\n",
      "    B-return_date.month_name      0.000     0.000     0.000         2\n",
      " B-return_time.period_of_day      0.000     0.000     0.000         1\n",
      "                B-round_trip      0.926     1.000     0.962        25\n",
      "                B-state_code      1.000     1.000     1.000         1\n",
      "                B-state_name      0.000     0.000     0.000         1\n",
      "         B-stoploc.city_name      0.955     1.000     0.977        21\n",
      "        B-toloc.airport_code      1.000     0.750     0.857         4\n",
      "        B-toloc.airport_name      1.000     0.500     0.667         4\n",
      "           B-toloc.city_name      0.979     0.974     0.976       424\n",
      "          B-toloc.state_code      1.000     1.000     1.000        10\n",
      "          B-toloc.state_name      1.000     0.714     0.833         7\n",
      "            B-transport_type      0.889     1.000     0.941         8\n",
      "              I-airline_name      0.977     1.000     0.988        42\n",
      "              I-airport_name      0.000     0.000     0.000         4\n",
      "      I-arrive_time.end_time      1.000     1.000     1.000         3\n",
      "          I-arrive_time.time      1.000     1.000     1.000        12\n",
      "                 I-city_name      0.750     0.750     0.750         4\n",
      "                I-class_type      0.826     1.000     0.905        19\n",
      "             I-cost_relative      1.000     1.000     1.000         6\n",
      "    I-depart_date.day_number      0.929     1.000     0.963        13\n",
      "      I-depart_time.end_time      1.000     1.000     1.000         1\n",
      " I-depart_time.period_of_day      0.000     0.000     0.000         1\n",
      "    I-depart_time.start_time      1.000     1.000     1.000         1\n",
      "          I-depart_time.time      1.000     0.971     0.986        35\n",
      "               I-fare_amount      1.000     1.000     1.000         6\n",
      "           I-fare_basis_code      0.000     0.000     0.000         1\n",
      "                I-flight_mod      0.000     0.000     0.000         2\n",
      "               I-flight_stop      1.000     1.000     1.000         2\n",
      "               I-flight_time      1.000     0.167     0.286         6\n",
      "      I-fromloc.airport_name      1.000     0.636     0.778        22\n",
      "         I-fromloc.city_name      0.919     0.966     0.942        59\n",
      "        I-fromloc.state_name      1.000     1.000     1.000         1\n",
      "          I-restriction_code      1.000     1.000     1.000         1\n",
      "    I-return_date.day_number      0.000     0.000     0.000         1\n",
      "                I-round_trip      0.962     1.000     0.980        25\n",
      "         I-stoploc.city_name      1.000     1.000     1.000         6\n",
      "        I-toloc.airport_name      0.600     1.000     0.750         3\n",
      "           I-toloc.city_name      0.965     0.973     0.969       112\n",
      "          I-toloc.state_name      1.000     1.000     1.000         1\n",
      "            I-transport_type      1.000     0.500     0.667         2\n",
      "                           O      0.988     0.993     0.991      4603\n",
      "                           X      1.000     0.995     0.997      2645\n",
      "\n",
      "                    accuracy                          0.981      9348\n",
      "                   macro avg      0.773     0.752     0.749      9348\n",
      "                weighted avg      0.981     0.981     0.980      9348\n",
      "\n",
      "len(y_true): 500\n",
      "preds.shape: (500, 47)\n",
      "attention_mask.shape: (500, 47)\n",
      "y_flat (flattened): 9348\n",
      "preds_flat.shape (flattened): (23500,)\n",
      "preds_flat.shape (masked): (9348,)\n",
      "SF NON-VERBOSE MACRO-F1: 0.749352084348367\n",
      "\n",
      "\n",
      "\n",
      "INTENT:\n",
      "                               precision    recall  f1-score   support\n",
      "\n",
      "            atis_abbreviation       0.89      0.94      0.91        17\n",
      "                atis_aircraft       0.92      1.00      0.96        11\n",
      "                 atis_airfare       0.84      1.00      0.92        38\n",
      "atis_airfare#atis_flight_time       0.00      0.00      0.00         1\n",
      "                 atis_airline       1.00      0.94      0.97        18\n",
      "                 atis_airport       1.00      0.67      0.80         3\n",
      "                atis_capacity       1.00      1.00      1.00         1\n",
      "                    atis_city       0.00      0.00      0.00         1\n",
      "                atis_distance       1.00      0.33      0.50         3\n",
      "                  atis_flight       0.99      0.99      0.99       357\n",
      "     atis_flight#atis_airfare       0.00      0.00      0.00         2\n",
      "               atis_flight_no       0.00      0.00      0.00         0\n",
      "             atis_flight_time       1.00      0.89      0.94         9\n",
      "             atis_ground_fare       1.00      1.00      1.00         3\n",
      "          atis_ground_service       1.00      1.00      1.00        25\n",
      "                atis_quantity       1.00      0.90      0.95        10\n",
      "             atis_restriction       1.00      1.00      1.00         1\n",
      "\n",
      "                     accuracy                           0.97       500\n",
      "                    macro avg       0.74      0.69      0.70       500\n",
      "                 weighted avg       0.97      0.97      0.97       500\n",
      "\n",
      "INTENT NON-VERBOSE MACRO-F1: 0.7020728873703106\n"
     ]
    }
   ],
   "source": [
    "probs_iob = predict_output[\"probs_iob\"]\n",
    "preds_iob = predict_output[\"preds_iob\"]\n",
    "attention_mask = predict_output[\"attention_mask\"]\n",
    "\n",
    "y_iob_tags_matrix_dev, y_intent_dev = y_dev\n",
    "    \n",
    "sequence_tagging_classification_report(y_iob_tags_matrix_dev, (probs_iob, preds_iob, attention_mask))\n",
    "print(\"SF NON-VERBOSE MACRO-F1:\", sequence_tagging_macro_f1(y_iob_tags_matrix_dev, \n",
    "                                                         (probs_iob, preds_iob, attention_mask)))\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\nINTENT:\")\n",
    "print(classification_report(y_intent_dev, predict_output[\"preds_intent\"]))\n",
    "print(\"INTENT NON-VERBOSE MACRO-F1:\", utils.safe_macro_f1(y_intent_dev,  predict_output[\"preds_intent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
